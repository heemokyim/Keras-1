---
layout: post
title:  "다중분류 해보기"
author: Taeyoung, Kim
date:   2017-02-04 10:00:00
categories: Keras
comments: true
---
본 강좌에서는 다중분류(Multiclass Classification)을 다층 퍼셉트론으로 해보겠습니다. 다음과 같은 순서로 진행하겠습니다.

1. 데이터셋 준비하기
1. 모델 구성하기
1. 모델 엮기
1. 모델 학습시키기
1. 모델 사용하기

---

### 데이터셋 준비하기

In this tutorial we will use the standard machine learning problem called the iris flowers dataset. This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species. The attributes for this dataset can be summarized as follows:
1. Sepal length in centimeters. 
2. Sepal width in centimeters. 
3. Petal length in centimeters. 
4. Petal width in centimeters. 
5. Class.

This is a multiclass classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling. Below is a sample of the first five of the 150 instances:

    5.1,3.5,1.4,0.2,Iris-setosa
    4.9,3.0,1.4,0.2,Iris-setosa
    4.7,3.2,1.3,0.2,Iris-setosa
    4.6,3.1,1.5,0.2,Iris-setosa
    5.0,3.6,1.4,0.2,Iris-setosa

[다운](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)


```python
# Binary Classification with Sonar Dataset: Standardized

import numpy
import pandas
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
```

    Using Theano backend.



```python
# load dataset
dataframe = pandas.read_csv("warehouse/iris.data", header=None)
dataset = dataframe.values
X = dataset[:,0:4].astype(float)
Y = dataset[:,4]
```

    Iris-setosa
    Iris-versicolor
    Iris-virginica

    Iris-setosa, Iris-versicolor, Iris-virginica 
    1, 0, 0
    0, 1, 0
    0, 0, 1


```python
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)
```

---

### 모델 구성하기

Dense 클래스를 사용하여 완전 연결 레이어(Fully Connected Layer)를 정의할 수 있다.

- 첫번째 인자 : 뉴런의 수
- 두번째 인자 : 네트워크 가중치(network weight) 초기화 방법
 - uniform : 균등분포 (uniform distribution)의 작은 난수들로 초기화 (0~0.05 사이)
 - normal : 가우시안 분포 (Gaussian distribution)로 생성된 작은 난수들로 초기화 
- 세번째 인자 : 활성화 함수(activation function) 지정
 - relu : rectifier 활성화 함수
 - sigmoid : sigmoid 활성화 함수
 - tanh : tanh 활성화 함수
 
마지막 레이어는 sigmoid 할성화 함수를 사용하는데, 이유는 결과가 0과 1사이로 나오는 것을 보장하며, 양성 클래스의 확률로 쉽게 매핑할 수 있기 때문이다. 또한 0.5 임계치(threshold)을 같은 클래스의 범주형 분류(hard classification)를 할 수 있다.

- 첫번째 은닉층(hidden layer)는 12개 뉴런을 가지고, 8개 입력을 받아들인다.
- 두번째 은닉층은 8개 뉴런을 가진다.
- 마지막 레이어는 클래스를 예측하는 1개의 뉴런을 가진다.


```python
# create model
model = Sequential()
model.add(Dense(4, input_dim=4, init= 'normal' , activation= 'relu' ))
model.add(Dense(3, init= 'normal' , activation= 'sigmoid' ))
```


```python
from IPython.display import SVG
from keras.utils.visualize_util import model_to_dot

SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))
```




![svg](output_9_0.svg)



![svg]({{ site.baseurl }}/posts_warehouse/2017-2-4-1.svg)

---

### 모델 엮기

컴파일 시에 정의해야하는 것들
- 가중치 세트를 평가하는 데 사용할 손실함수(loss function)
 - binary_crossentropy : 이진 분류를 위한 logarithmic loss
- 네트워크의 다른 가중치를 검객하는 데 사용되는 최적화 알고리즘
 - adam : 효율적인 경사 하강법(gradient descent) 알고리즘
- 학습과정에서 수집하기 싶은 측정 기준


```python
# Compile model
model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])
```

---

### 모델 학습시키기

- nb_epoch : 데이터셋에 대한 반복 횟수
- batch_size : 네트워크에서 가중치 개갱신 전에 평가되는 인스턴스의 수


```python
# Fit the model
model.fit(X, dummy_y, nb_epoch=200, batch_size=5) # nb_epoch 200
```

    Epoch 1/200
    150/150 [==============================] - 0s - loss: 1.0983 - acc: 0.2733     
    Epoch 2/200
    150/150 [==============================] - 0s - loss: 1.0945 - acc: 0.2133     
    Epoch 3/200
    150/150 [==============================] - 0s - loss: 1.0880 - acc: 0.3133     
    Epoch 4/200
    150/150 [==============================] - 0s - loss: 1.0792 - acc: 0.3333     
    Epoch 5/200
    150/150 [==============================] - 0s - loss: 1.0652 - acc: 0.3333     
    Epoch 6/200
    150/150 [==============================] - 0s - loss: 1.0496 - acc: 0.3333     
    Epoch 7/200
    150/150 [==============================] - 0s - loss: 1.0315 - acc: 0.3333     
    Epoch 8/200
    150/150 [==============================] - 0s - loss: 1.0115 - acc: 0.3333     
    Epoch 9/200
    150/150 [==============================] - 0s - loss: 0.9903 - acc: 0.3333     
    Epoch 10/200
    150/150 [==============================] - 0s - loss: 0.9676 - acc: 0.3600     
    Epoch 11/200
    150/150 [==============================] - 0s - loss: 0.9435 - acc: 0.4933     
    Epoch 12/200
    150/150 [==============================] - 0s - loss: 0.9221 - acc: 0.5800     
    Epoch 13/200
    150/150 [==============================] - 0s - loss: 0.9029 - acc: 0.5533     
    Epoch 14/200
    150/150 [==============================] - 0s - loss: 0.8858 - acc: 0.6200     
    Epoch 15/200
    150/150 [==============================] - 0s - loss: 0.8718 - acc: 0.6267     
    Epoch 16/200
    150/150 [==============================] - 0s - loss: 0.8598 - acc: 0.6333     
    Epoch 17/200
    150/150 [==============================] - 0s - loss: 0.8491 - acc: 0.6333     
    Epoch 18/200
    150/150 [==============================] - 0s - loss: 0.8390 - acc: 0.6600     
    Epoch 19/200
    150/150 [==============================] - 0s - loss: 0.8307 - acc: 0.6533     
    Epoch 20/200
    150/150 [==============================] - 0s - loss: 0.8226 - acc: 0.6667     
    Epoch 21/200
    150/150 [==============================] - 0s - loss: 0.8146 - acc: 0.6667     
    Epoch 22/200
    150/150 [==============================] - 0s - loss: 0.8076 - acc: 0.6667     
    Epoch 23/200
    150/150 [==============================] - 0s - loss: 0.8009 - acc: 0.6667     
    Epoch 24/200
    150/150 [==============================] - 0s - loss: 0.7944 - acc: 0.6667     
    Epoch 25/200
    150/150 [==============================] - 0s - loss: 0.7887 - acc: 0.6667     
    Epoch 26/200
    150/150 [==============================] - 0s - loss: 0.7826 - acc: 0.6667     
    Epoch 27/200
    150/150 [==============================] - 0s - loss: 0.7768 - acc: 0.6667     
    Epoch 28/200
    150/150 [==============================] - 0s - loss: 0.7714 - acc: 0.6667     
    Epoch 29/200
    150/150 [==============================] - 0s - loss: 0.7671 - acc: 0.6667     
    Epoch 30/200
    150/150 [==============================] - 0s - loss: 0.7607 - acc: 0.6667     
    Epoch 31/200
    150/150 [==============================] - 0s - loss: 0.7556 - acc: 0.6667     
    Epoch 32/200
    150/150 [==============================] - 0s - loss: 0.7507 - acc: 0.6667     
    Epoch 33/200
    150/150 [==============================] - 0s - loss: 0.7458 - acc: 0.6667     
    Epoch 34/200
    150/150 [==============================] - 0s - loss: 0.7407 - acc: 0.6667     
    Epoch 35/200
    150/150 [==============================] - 0s - loss: 0.7364 - acc: 0.6667     
    Epoch 36/200
    150/150 [==============================] - 0s - loss: 0.7321 - acc: 0.6667     
    Epoch 37/200
    150/150 [==============================] - 0s - loss: 0.7265 - acc: 0.6667     
    Epoch 38/200
    150/150 [==============================] - 0s - loss: 0.7228 - acc: 0.6667     
    Epoch 39/200
    150/150 [==============================] - 0s - loss: 0.7174 - acc: 0.6667     
    Epoch 40/200
    150/150 [==============================] - 0s - loss: 0.7128 - acc: 0.6667     
    Epoch 41/200
    150/150 [==============================] - 0s - loss: 0.7086 - acc: 0.6667     
    Epoch 42/200
    150/150 [==============================] - 0s - loss: 0.7044 - acc: 0.6667     
    Epoch 43/200
    150/150 [==============================] - 0s - loss: 0.7000 - acc: 0.6667     
    Epoch 44/200
    150/150 [==============================] - 0s - loss: 0.6955 - acc: 0.6667     
    Epoch 45/200
    150/150 [==============================] - 0s - loss: 0.6910 - acc: 0.6667     
    Epoch 46/200
    150/150 [==============================] - 0s - loss: 0.6885 - acc: 0.6667     
    Epoch 47/200
    150/150 [==============================] - 0s - loss: 0.6826 - acc: 0.6733     
    Epoch 48/200
    150/150 [==============================] - 0s - loss: 0.6787 - acc: 0.6733     
    Epoch 49/200
    150/150 [==============================] - 0s - loss: 0.6748 - acc: 0.6667     
    Epoch 50/200
    150/150 [==============================] - 0s - loss: 0.6706 - acc: 0.6733     
    Epoch 51/200
    150/150 [==============================] - 0s - loss: 0.6670 - acc: 0.6733     
    Epoch 52/200
    150/150 [==============================] - 0s - loss: 0.6632 - acc: 0.6800     
    Epoch 53/200
    150/150 [==============================] - 0s - loss: 0.6591 - acc: 0.6733     
    Epoch 54/200
    150/150 [==============================] - 0s - loss: 0.6552 - acc: 0.6733     
    Epoch 55/200
    150/150 [==============================] - 0s - loss: 0.6512 - acc: 0.6867     
    Epoch 56/200
    150/150 [==============================] - 0s - loss: 0.6475 - acc: 0.6933     
    Epoch 57/200
    150/150 [==============================] - 0s - loss: 0.6439 - acc: 0.6933     
    Epoch 58/200
    150/150 [==============================] - 0s - loss: 0.6400 - acc: 0.6933     
    Epoch 59/200
    150/150 [==============================] - 0s - loss: 0.6367 - acc: 0.6933     
    Epoch 60/200
    150/150 [==============================] - 0s - loss: 0.6332 - acc: 0.6933     
    Epoch 61/200
    150/150 [==============================] - 0s - loss: 0.6302 - acc: 0.7067     
    Epoch 62/200
    150/150 [==============================] - 0s - loss: 0.6266 - acc: 0.7067     
    Epoch 63/200
    150/150 [==============================] - 0s - loss: 0.6226 - acc: 0.6933     
    Epoch 64/200
    150/150 [==============================] - 0s - loss: 0.6193 - acc: 0.6933     
    Epoch 65/200
    150/150 [==============================] - 0s - loss: 0.6162 - acc: 0.6933     
    Epoch 66/200
    150/150 [==============================] - 0s - loss: 0.6128 - acc: 0.6933     
    Epoch 67/200
    150/150 [==============================] - 0s - loss: 0.6095 - acc: 0.6933     
    Epoch 68/200
    150/150 [==============================] - 0s - loss: 0.6066 - acc: 0.7200     
    Epoch 69/200
    150/150 [==============================] - 0s - loss: 0.6039 - acc: 0.7133     
    Epoch 70/200
    150/150 [==============================] - 0s - loss: 0.6002 - acc: 0.7200     
    Epoch 71/200
    150/150 [==============================] - 0s - loss: 0.5976 - acc: 0.7133     
    Epoch 72/200
    150/150 [==============================] - 0s - loss: 0.5935 - acc: 0.7133     
    Epoch 73/200
    150/150 [==============================] - 0s - loss: 0.5904 - acc: 0.7200     
    Epoch 74/200
    150/150 [==============================] - 0s - loss: 0.5896 - acc: 0.7467     
    Epoch 75/200
    150/150 [==============================] - 0s - loss: 0.5856 - acc: 0.7267     
    Epoch 76/200
    150/150 [==============================] - 0s - loss: 0.5829 - acc: 0.7133     
    Epoch 77/200
    150/150 [==============================] - 0s - loss: 0.5785 - acc: 0.7133     
    Epoch 78/200
    150/150 [==============================] - 0s - loss: 0.5767 - acc: 0.7533     
    Epoch 79/200
    150/150 [==============================] - 0s - loss: 0.5732 - acc: 0.7400     
    Epoch 80/200
    150/150 [==============================] - 0s - loss: 0.5706 - acc: 0.7667     
    Epoch 81/200
    150/150 [==============================] - 0s - loss: 0.5681 - acc: 0.7667     
    Epoch 82/200
    150/150 [==============================] - 0s - loss: 0.5653 - acc: 0.7400     
    Epoch 83/200
    150/150 [==============================] - 0s - loss: 0.5642 - acc: 0.8133     
    Epoch 84/200
    150/150 [==============================] - 0s - loss: 0.5588 - acc: 0.7733     
    Epoch 85/200
    150/150 [==============================] - 0s - loss: 0.5573 - acc: 0.7533     
    Epoch 86/200
    150/150 [==============================] - 0s - loss: 0.5542 - acc: 0.7667     
    Epoch 87/200
    150/150 [==============================] - 0s - loss: 0.5514 - acc: 0.7733     
    Epoch 88/200
    150/150 [==============================] - 0s - loss: 0.5483 - acc: 0.7733     
    Epoch 89/200
    150/150 [==============================] - 0s - loss: 0.5463 - acc: 0.8000     
    Epoch 90/200
    150/150 [==============================] - 0s - loss: 0.5442 - acc: 0.7733     
    Epoch 91/200
    150/150 [==============================] - 0s - loss: 0.5405 - acc: 0.8000     
    Epoch 92/200
    150/150 [==============================] - 0s - loss: 0.5376 - acc: 0.8067     
    Epoch 93/200
    150/150 [==============================] - 0s - loss: 0.5354 - acc: 0.8133     
    Epoch 94/200
    150/150 [==============================] - 0s - loss: 0.5328 - acc: 0.8333     
    Epoch 95/200
    150/150 [==============================] - 0s - loss: 0.5305 - acc: 0.7733     
    Epoch 96/200
    150/150 [==============================] - 0s - loss: 0.5291 - acc: 0.8400     
    Epoch 97/200
    150/150 [==============================] - 0s - loss: 0.5249 - acc: 0.8200     
    Epoch 98/200
    150/150 [==============================] - 0s - loss: 0.5216 - acc: 0.8267     
    Epoch 99/200
    150/150 [==============================] - 0s - loss: 0.5191 - acc: 0.8333     
    Epoch 100/200
    150/150 [==============================] - 0s - loss: 0.5161 - acc: 0.8333     
    Epoch 101/200
    150/150 [==============================] - 0s - loss: 0.5142 - acc: 0.8533     
    Epoch 102/200
    150/150 [==============================] - 0s - loss: 0.5123 - acc: 0.8667     
    Epoch 103/200
    150/150 [==============================] - 0s - loss: 0.5083 - acc: 0.8533     
    Epoch 104/200
    150/150 [==============================] - 0s - loss: 0.5083 - acc: 0.8133     
    Epoch 105/200
    150/150 [==============================] - 0s - loss: 0.5032 - acc: 0.8467     
    Epoch 106/200
    150/150 [==============================] - 0s - loss: 0.4996 - acc: 0.8800     
    Epoch 107/200
    150/150 [==============================] - 0s - loss: 0.4967 - acc: 0.9000     
    Epoch 108/200
    150/150 [==============================] - 0s - loss: 0.4938 - acc: 0.8800     
    Epoch 109/200
    150/150 [==============================] - 0s - loss: 0.4911 - acc: 0.8800     
    Epoch 110/200
    150/150 [==============================] - 0s - loss: 0.4877 - acc: 0.8933     
    Epoch 111/200
    150/150 [==============================] - 0s - loss: 0.4866 - acc: 0.9200     
    Epoch 112/200
    150/150 [==============================] - 0s - loss: 0.4812 - acc: 0.8933     
    Epoch 113/200
    150/150 [==============================] - 0s - loss: 0.4783 - acc: 0.8933     
    Epoch 114/200
    150/150 [==============================] - 0s - loss: 0.4764 - acc: 0.9133     
    Epoch 115/200
    150/150 [==============================] - 0s - loss: 0.4737 - acc: 0.9000     
    Epoch 116/200
    150/150 [==============================] - 0s - loss: 0.4687 - acc: 0.9067     
    Epoch 117/200
    150/150 [==============================] - 0s - loss: 0.4651 - acc: 0.9400     
    Epoch 118/200
    150/150 [==============================] - 0s - loss: 0.4615 - acc: 0.9133     
    Epoch 119/200
    150/150 [==============================] - 0s - loss: 0.4573 - acc: 0.9667     
    Epoch 120/200
    150/150 [==============================] - 0s - loss: 0.4547 - acc: 0.9400     
    Epoch 121/200
    150/150 [==============================] - 0s - loss: 0.4495 - acc: 0.9600     
    Epoch 122/200
    150/150 [==============================] - 0s - loss: 0.4456 - acc: 0.9667     
    Epoch 123/200
    150/150 [==============================] - 0s - loss: 0.4414 - acc: 0.9667     
    Epoch 124/200
    150/150 [==============================] - 0s - loss: 0.4376 - acc: 0.9667     
    Epoch 125/200
    150/150 [==============================] - 0s - loss: 0.4337 - acc: 0.9667     
    Epoch 126/200
    150/150 [==============================] - 0s - loss: 0.4305 - acc: 0.9667     
    Epoch 127/200
    150/150 [==============================] - 0s - loss: 0.4251 - acc: 0.9667     
    Epoch 128/200
    150/150 [==============================] - 0s - loss: 0.4212 - acc: 0.9667     
    Epoch 129/200
    150/150 [==============================] - 0s - loss: 0.4167 - acc: 0.9667     
    Epoch 130/200
    150/150 [==============================] - 0s - loss: 0.4138 - acc: 0.9667     
    Epoch 131/200
    150/150 [==============================] - 0s - loss: 0.4093 - acc: 0.9667     
    Epoch 132/200
    150/150 [==============================] - 0s - loss: 0.4044 - acc: 0.9667     
    Epoch 133/200
    150/150 [==============================] - 0s - loss: 0.4010 - acc: 0.9667     
    Epoch 134/200
    150/150 [==============================] - 0s - loss: 0.3982 - acc: 0.9667     
    Epoch 135/200
    150/150 [==============================] - 0s - loss: 0.3927 - acc: 0.9667     
    Epoch 136/200
    150/150 [==============================] - 0s - loss: 0.3893 - acc: 0.9667     
    Epoch 137/200
    150/150 [==============================] - 0s - loss: 0.3856 - acc: 0.9667     
    Epoch 138/200
    150/150 [==============================] - 0s - loss: 0.3829 - acc: 0.9667     
    Epoch 139/200
    150/150 [==============================] - 0s - loss: 0.3800 - acc: 0.9667     
    Epoch 140/200
    150/150 [==============================] - 0s - loss: 0.3750 - acc: 0.9667     
    Epoch 141/200
    150/150 [==============================] - 0s - loss: 0.3718 - acc: 0.9667     
    Epoch 142/200
    150/150 [==============================] - 0s - loss: 0.3687 - acc: 0.9667     
    Epoch 143/200
    150/150 [==============================] - 0s - loss: 0.3654 - acc: 0.9667     
    Epoch 144/200
    150/150 [==============================] - 0s - loss: 0.3614 - acc: 0.9667     
    Epoch 145/200
    150/150 [==============================] - 0s - loss: 0.3600 - acc: 0.9667     
    Epoch 146/200
    150/150 [==============================] - 0s - loss: 0.3552 - acc: 0.9667     
    Epoch 147/200
    150/150 [==============================] - 0s - loss: 0.3514 - acc: 0.9667     
    Epoch 148/200
    150/150 [==============================] - 0s - loss: 0.3484 - acc: 0.9667     
    Epoch 149/200
    150/150 [==============================] - 0s - loss: 0.3463 - acc: 0.9667     
    Epoch 150/200
    150/150 [==============================] - 0s - loss: 0.3429 - acc: 0.9667     
    Epoch 151/200
    150/150 [==============================] - 0s - loss: 0.3388 - acc: 0.9667     
    Epoch 152/200
    150/150 [==============================] - 0s - loss: 0.3364 - acc: 0.9667     
    Epoch 153/200
    150/150 [==============================] - 0s - loss: 0.3326 - acc: 0.9667     
    Epoch 154/200
    150/150 [==============================] - 0s - loss: 0.3300 - acc: 0.9667     
    Epoch 155/200
    150/150 [==============================] - 0s - loss: 0.3269 - acc: 0.9667     
    Epoch 156/200
    150/150 [==============================] - 0s - loss: 0.3273 - acc: 0.9667     
    Epoch 157/200
    150/150 [==============================] - 0s - loss: 0.3209 - acc: 0.9667     
    Epoch 158/200
    150/150 [==============================] - 0s - loss: 0.3190 - acc: 0.9667     
    Epoch 159/200
    150/150 [==============================] - 0s - loss: 0.3151 - acc: 0.9667     
    Epoch 160/200
    150/150 [==============================] - 0s - loss: 0.3122 - acc: 0.9667     
    Epoch 161/200
    150/150 [==============================] - 0s - loss: 0.3093 - acc: 0.9667     
    Epoch 162/200
    150/150 [==============================] - 0s - loss: 0.3068 - acc: 0.9667     
    Epoch 163/200
    150/150 [==============================] - 0s - loss: 0.3040 - acc: 0.9667     
    Epoch 164/200
    150/150 [==============================] - 0s - loss: 0.3014 - acc: 0.9667     
    Epoch 165/200
    150/150 [==============================] - 0s - loss: 0.2984 - acc: 0.9667     
    Epoch 166/200
    150/150 [==============================] - 0s - loss: 0.2957 - acc: 0.9667     
    Epoch 167/200
    150/150 [==============================] - 0s - loss: 0.2927 - acc: 0.9667     
    Epoch 168/200
    150/150 [==============================] - 0s - loss: 0.2902 - acc: 0.9667     
    Epoch 169/200
    150/150 [==============================] - 0s - loss: 0.2877 - acc: 0.9667     
    Epoch 170/200
    150/150 [==============================] - 0s - loss: 0.2847 - acc: 0.9667     
    Epoch 171/200
    150/150 [==============================] - 0s - loss: 0.2840 - acc: 0.9667     
    Epoch 172/200
    150/150 [==============================] - 0s - loss: 0.2809 - acc: 0.9667     
    Epoch 173/200
    150/150 [==============================] - 0s - loss: 0.2784 - acc: 0.9667     
    Epoch 174/200
    150/150 [==============================] - 0s - loss: 0.2762 - acc: 0.9667     
    Epoch 175/200
    150/150 [==============================] - 0s - loss: 0.2740 - acc: 0.9667     
    Epoch 176/200
    150/150 [==============================] - 0s - loss: 0.2708 - acc: 0.9667     
    Epoch 177/200
    150/150 [==============================] - 0s - loss: 0.2688 - acc: 0.9667     
    Epoch 178/200
    150/150 [==============================] - 0s - loss: 0.2676 - acc: 0.9667     
    Epoch 179/200
    150/150 [==============================] - 0s - loss: 0.2636 - acc: 0.9667     
    Epoch 180/200
    150/150 [==============================] - 0s - loss: 0.2615 - acc: 0.9733     
    Epoch 181/200
    150/150 [==============================] - 0s - loss: 0.2585 - acc: 0.9733     
    Epoch 182/200
    150/150 [==============================] - 0s - loss: 0.2581 - acc: 0.9667     
    Epoch 183/200
    150/150 [==============================] - 0s - loss: 0.2537 - acc: 0.9733     
    Epoch 184/200
    150/150 [==============================] - 0s - loss: 0.2526 - acc: 0.9667     
    Epoch 185/200
    150/150 [==============================] - 0s - loss: 0.2493 - acc: 0.9733     
    Epoch 186/200
    150/150 [==============================] - 0s - loss: 0.2475 - acc: 0.9667     
    Epoch 187/200
    150/150 [==============================] - 0s - loss: 0.2451 - acc: 0.9733     
    Epoch 188/200
    150/150 [==============================] - 0s - loss: 0.2451 - acc: 0.9667     
    Epoch 189/200
    150/150 [==============================] - 0s - loss: 0.2417 - acc: 0.9733     
    Epoch 190/200
    150/150 [==============================] - 0s - loss: 0.2392 - acc: 0.9667     
    Epoch 191/200
    150/150 [==============================] - 0s - loss: 0.2374 - acc: 0.9667     
    Epoch 192/200
    150/150 [==============================] - 0s - loss: 0.2352 - acc: 0.9667     
    Epoch 193/200
    150/150 [==============================] - 0s - loss: 0.2334 - acc: 0.9733     
    Epoch 194/200
    150/150 [==============================] - 0s - loss: 0.2324 - acc: 0.9733     
    Epoch 195/200
    150/150 [==============================] - 0s - loss: 0.2305 - acc: 0.9733     
    Epoch 196/200
    150/150 [==============================] - 0s - loss: 0.2277 - acc: 0.9733     
    Epoch 197/200
    150/150 [==============================] - 0s - loss: 0.2258 - acc: 0.9733     
    Epoch 198/200
    150/150 [==============================] - 0s - loss: 0.2255 - acc: 0.9667     
    Epoch 199/200
    150/150 [==============================] - 0s - loss: 0.2211 - acc: 0.9733     
    Epoch 200/200
    150/150 [==============================] - 0s - loss: 0.2205 - acc: 0.9733     





    <keras.callbacks.History at 0x115c48a50>



---

### 모델 사용하기


```python
# evaliuate
scores = model.evaluate(X, dummy_y)

print("")
print("%s: %.2f%%" %(model.metrics_names[1], scores[1]*100))
```

     32/208 [===>..........................] - ETA: 0s
    acc: 94.23%


---

### 같이 보기

* [강좌 목차](https://tykimos.github.io/Keras/2017/01/27/Keras_Lecture_Plan/)
* 이전 : [딥러닝 이야기/레이어 이야기](https://tykimos.github.io/Keras/2017/01/27/Layer_Talk/)
* 다음 : [딥러닝 기본 실습/컨볼루션 신경망 모델 만들어보기](https://tykimos.github.io/Keras/2017/02/04/CNN_Getting_Started/)


```python

```
