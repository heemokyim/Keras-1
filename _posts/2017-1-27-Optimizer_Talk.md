---
layout: post
title:  "최적화방법 이야기"
author: Taeyoung, Kim
date:   2017-01-27 23:07:00
categories: Keras
comments: true
---
* SGD : Stochastic gradient descent, with support for momentum, learning rate decay, and Nesterov momentum.
* RMSprop : RNN에 주로 사용됨
* Adam
* Adagrad
* Adadelta
* Adamax
* Nadam
* TFOptimizer


```python

```
