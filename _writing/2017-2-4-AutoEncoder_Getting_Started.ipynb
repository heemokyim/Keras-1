{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"오토인코더 모델 만들어보기\"\n",
    "author: Taeyoung, Kim\n",
    "date:   2017-02-04 00:00:00\n",
    "categories: Keras\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 강좌에서는 오코인코더 모델을 만들어보겠습니다.\n",
    "\n",
    "CNN은 라벨이 있는 감독 분류이나 현업 자료에서는 라벨이 지정되지 않은 경우가 많으며, 라벨링하는 작업도 상당히 노동집약적이고 비용이 듭니다. 이를 해결하기 위해 무감독학습 연구도 발전되고 있습니다. 무감독학습은 라벨이 없기 때문에 데이터의 전체적 특성을 학습하게 되므로, 입력데이터를 재생성하는 생성적 학습을 다루게 됩니다. 다음 목록들의 모델이 생성 모델에 속합니다.\n",
    "\n",
    "* Deep Belief Network (DBN)\n",
    "* Deep Boltzmann Machine (DBM)\n",
    "* Variational Auto-encoder (VAE)\n",
    "* Generative Adversarial Network (GAN)\n",
    "\n",
    "In this tutorial, we will answer some common questions about autoencoders, and we will cover code examples of the following models:\n",
    "\n",
    "a simple autoencoder based on a fully-connected layer\n",
    "a sparse autoencoder\n",
    "a deep fully-connected autoencoder\n",
    "a deep convolutional autoencoder\n",
    "an image denoising model\n",
    "a sequence-to-sequence autoencoder\n",
    "a variational autoencoder\n",
    "\n",
    "\n",
    "\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n",
    "\n",
    "1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n",
    "\n",
    "2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n",
    "\n",
    "3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n",
    "\n",
    "To build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent. It's simple! And you don't even need to understand any of these words to start using autoencoders in practice.\n",
    "\n",
    "Are they good at data compression?\n",
    "\n",
    "Usually, not really. In picture compression for instance, it is pretty difficult to train an autoencoder that does a better job than a basic algorithm like JPEG, and typically the only way it can be achieved is by restricting yourself to a very specific type of picture (e.g. one for which JPEG does not do a good job). The fact that autoencoders are data-specific makes them generally impractical for real-world data compression problems: you can only use them on data that is similar to what they were trained on, and making them more general thus requires lots of training data. But future advances might change this, who knows.\n",
    "\n",
    "What are autoencoders good for?\n",
    "\n",
    "They are rarely used in practical applications. In 2012 they briefly found an application in greedy layer-wise pretraining for deep convolutional neural networks [1], but this quickly fell out of fashion as we started realizing that better random weight initialization schemes were sufficient for training deep networks from scratch. In 2014, batch normalization [2] started allowing for even deeper networks, and from late 2015 we could train arbitrarily deep networks from scratch using residual learning [3].\n",
    "\n",
    "Today two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n",
    "\n",
    "For 2D visualization specifically, t-SNE (pronounced \"tee-snee\") is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32 dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and is available on Github. Otherwise scikit-learn also has a simple and practical implementation.\n",
    "\n",
    "So what's the big deal with autoencoders?\n",
    "\n",
    "Their main claim to fame comes from being featured in many introductory machine learning classes available online. As a result, a lot of newcomers to the field absolutely love autoencoders and can't get enough of them. This is the reason why this tutorial exists!\n",
    "\n",
    "Otherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a self-supervised technique, a specific instance of supervised learning where the targets are generated from the input data. In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that's where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here. At this point there is significant evidence that focusing on the reconstruction of a picture at the pixel level, for instance, is not conductive to learning interesting, abstract features of the kind that label-supervized learning induces (where targets are fairly abstract concepts \"invented\" by humans such as \"dog\", \"car\"...). In fact, one may argue that the best features in this regard are those that are the worst at exact input reconstruction while achieving high performance on the main task that you are interested in (classification, localization, etc).\n",
    "\n",
    "In self-supervized learning applied to vision, a potentially fruitful alternative to autoencoder-style input reconstruction is the use of toy tasks such as jigsaw puzzle solving, or detail-context matching (being able to match high-resolution but small patches of pictures with low-resolution versions of the pictures they are extracted from). The following paper investigates jigsaw puzzle solving and makes for a very interesting read: Noroozi and Favaro (2016) Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. Such tasks are providing the model with built-in assumptions about the input data which are missing in traditional autoencoders, such as \"visual macro-structure matters more than pixel-level details\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the simplest possible autoencoder\n",
    "\n",
    "We'll start simple, with a single fully-connected neural layer as encoder and as decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input=input_img, output=decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a separate encoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input=input_img, output=encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the decoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder to reconstruct MNIST digits.\n",
    "\n",
    "First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print x_train.shape\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder for 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.3453 - val_loss: 0.2694\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.2614 - val_loss: 0.2501\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s - loss: 0.2406 - val_loss: 0.2286\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.2202 - val_loss: 0.2098\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.2045 - val_loss: 0.1970\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1940 - val_loss: 0.1882\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.1862 - val_loss: 0.1814\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.1796 - val_loss: 0.1751\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1738 - val_loss: 0.1695\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1683 - val_loss: 0.1643\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.1634 - val_loss: 0.1595\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1589 - val_loss: 0.1554\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1549 - val_loss: 0.1514\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 9s - loss: 0.1513 - val_loss: 0.1481\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1481 - val_loss: 0.1452\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 9s - loss: 0.1452 - val_loss: 0.1425\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1426 - val_loss: 0.1398\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 7s - loss: 0.1401 - val_loss: 0.1374\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1378 - val_loss: 0.1352\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1356 - val_loss: 0.1331\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1334 - val_loss: 0.1309\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 8s - loss: 0.1314 - val_loss: 0.1289\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s - loss: 0.1293 - val_loss: 0.1269\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s - loss: 0.1274 - val_loss: 0.1250\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s - loss: 0.1255 - val_loss: 0.1232\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s - loss: 0.1237 - val_loss: 0.1214\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s - loss: 0.1220 - val_loss: 0.1197\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1204 - val_loss: 0.1182\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1189 - val_loss: 0.1167\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1175 - val_loss: 0.1153\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 690s - loss: 0.1162 - val_loss: 0.1141\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s - loss: 0.1149 - val_loss: 0.1128\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 3s - loss: 0.1138 - val_loss: 0.1118\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 3s - loss: 0.1127 - val_loss: 0.1107\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 3s - loss: 0.1118 - val_loss: 0.1098\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 3s - loss: 0.1108 - val_loss: 0.1088\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 3s - loss: 0.1100 - val_loss: 0.1080\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1091 - val_loss: 0.1072\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1084 - val_loss: 0.1065\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1077 - val_loss: 0.1058\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1070 - val_loss: 0.1051\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1064 - val_loss: 0.1045\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 3s - loss: 0.1058 - val_loss: 0.1039\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s - loss: 0.1052 - val_loss: 0.1034\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1047 - val_loss: 0.1029\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1042 - val_loss: 0.1024\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1038 - val_loss: 0.1020\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1033 - val_loss: 0.1016\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1029 - val_loss: 0.1012\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s - loss: 0.1025 - val_loss: 0.1008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10b068bd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                nb_epoch=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs, the autoencoder seems to reach a stable train/test loss value of about 0.11. We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "You must compile your model before using it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d2a6037c896d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# encode and decode some digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# note that we take them from the *test* set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoded_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdecoded_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tykimos/Projects/insdeep_tb/venv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         return self._predict_loop(f, ins,\n",
      "\u001b[0;32m/Users/tykimos/Projects/insdeep_tb/venv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_make_predict_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must compile your model before using it.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: You must compile your model before using it."
     ]
    }
   ],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 305.99 296.00\" width=\"306pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-292 301.9902,-292 301.9902,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4390123856 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4390123856</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 297.9902,-287.5 297.9902,-243.5 0,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.3364\" y=\"-261.3\">dense_input_1 (InputLayer)</text>\n",
       "<polyline fill=\"none\" points=\"172.6729,-243.5 172.6729,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200.5073\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"172.6729,-265.5 228.3418,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200.5073\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"228.3418,-243.5 228.3418,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.166\" y=\"-272.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"228.3418,-265.5 297.9902,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.166\" y=\"-250.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 4390123600 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4390123600</title>\n",
       "<polygon fill=\"none\" points=\"27.9932,-162.5 27.9932,-206.5 269.9971,-206.5 269.9971,-162.5 27.9932,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.8364\" y=\"-180.3\">dense_1 (Dense)</text>\n",
       "<polyline fill=\"none\" points=\"137.6797,-162.5 137.6797,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.5142\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137.6797,-184.5 193.3486,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.5142\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"193.3486,-162.5 193.3486,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.6729\" y=\"-191.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"193.3486,-184.5 269.9971,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.6729\" y=\"-169.3\">(None, 12)</text>\n",
       "</g>\n",
       "<!-- 4390123856&#45;&gt;4390123600 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4390123856-&gt;4390123600</title>\n",
       "<path d=\"M148.9951,-243.3664C148.9951,-235.1516 148.9951,-225.6579 148.9951,-216.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"152.4952,-216.6068 148.9951,-206.6068 145.4952,-216.6069 152.4952,-216.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4390123728 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4390123728</title>\n",
       "<polygon fill=\"none\" points=\"27.9932,-81.5 27.9932,-125.5 269.9971,-125.5 269.9971,-81.5 27.9932,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.8364\" y=\"-99.3\">dense_2 (Dense)</text>\n",
       "<polyline fill=\"none\" points=\"137.6797,-81.5 137.6797,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.5142\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137.6797,-103.5 193.3486,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.5142\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"193.3486,-81.5 193.3486,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.6729\" y=\"-110.3\">(None, 12)</text>\n",
       "<polyline fill=\"none\" points=\"193.3486,-103.5 269.9971,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.6729\" y=\"-88.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 4390123600&#45;&gt;4390123728 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4390123600-&gt;4390123728</title>\n",
       "<path d=\"M148.9951,-162.3664C148.9951,-154.1516 148.9951,-144.6579 148.9951,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"152.4952,-135.6068 148.9951,-125.6068 145.4952,-135.6069 152.4952,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4391334288 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4391334288</title>\n",
       "<polygon fill=\"none\" points=\"31.4932,-.5 31.4932,-44.5 266.4971,-44.5 266.4971,-.5 31.4932,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.3364\" y=\"-18.3\">dense_3 (Dense)</text>\n",
       "<polyline fill=\"none\" points=\"141.1797,-.5 141.1797,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.0142\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"141.1797,-22.5 196.8486,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.0142\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"196.8486,-.5 196.8486,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.6729\" y=\"-29.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"196.8486,-22.5 266.4971,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.6729\" y=\"-7.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 4390123728&#45;&gt;4391334288 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4390123728-&gt;4391334288</title>\n",
       "<path d=\"M148.9951,-81.3664C148.9951,-73.1516 148.9951,-63.6579 148.9951,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"152.4952,-54.6068 148.9951,-44.6068 145.4952,-54.6069 152.4952,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "# brew install graphviz\n",
    "# pip uninstall -y pydot\n",
    "# pip install pydot-ng\n",
    "\n",
    "SVG(model_to_dot(autoencoder, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svg]({{ site.baseurl }}/posts_warehouse/2017-2-4-1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 엮기\n",
    "\n",
    "컴파일 시에 정의해야하는 것들\n",
    "- 가중치 세트를 평가하는 데 사용할 손실함수(loss function)\n",
    " - binary_crossentropy : 이진 분류를 위한 logarithmic loss\n",
    "- 네트워크의 다른 가중치를 검객하는 데 사용되는 최적화 알고리즘\n",
    " - adam : 효율적인 경사 하강법(gradient descent) 알고리즘\n",
    "- 학습과정에서 수집하기 싶은 측정 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습시키기\n",
    "\n",
    "- nb_epoch : 데이터셋에 대한 반복 횟수\n",
    "- batch_size : 네트워크에서 가중치 개갱신 전에 평가되는 인스턴스의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "768/768 [==============================] - 0s - loss: 0.6826 - acc: 0.6328     \n",
      "Epoch 2/10\n",
      "768/768 [==============================] - 0s - loss: 0.6590 - acc: 0.6510     \n",
      "Epoch 3/10\n",
      "768/768 [==============================] - 0s - loss: 0.6475 - acc: 0.6549     \n",
      "Epoch 4/10\n",
      "768/768 [==============================] - 0s - loss: 0.6416 - acc: 0.6615     \n",
      "Epoch 5/10\n",
      "768/768 [==============================] - 0s - loss: 0.6216 - acc: 0.6745     \n",
      "Epoch 6/10\n",
      "768/768 [==============================] - 0s - loss: 0.6128 - acc: 0.6680     \n",
      "Epoch 7/10\n",
      "768/768 [==============================] - 0s - loss: 0.6018 - acc: 0.6927     \n",
      "Epoch 8/10\n",
      "768/768 [==============================] - 0s - loss: 0.5962 - acc: 0.6927     \n",
      "Epoch 9/10\n",
      "768/768 [==============================] - 0s - loss: 0.5991 - acc: 0.6953     \n",
      "Epoch 10/10\n",
      "768/768 [==============================] - 0s - loss: 0.5920 - acc: 0.6927     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x106873b90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s     \n",
      "acc: 70.18%\n"
     ]
    }
   ],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 같이 보기\n",
    "\n",
    "* [강좌 목차](https://tykimos.github.io/Keras/2017/01/27/Keras_Lecture_Plan/)\n",
    "* 이전 : [딥러닝 이야기/레이어 이야기](https://tykimos.github.io/Keras/2017/01/27/Layer_Talk/)\n",
    "* 다음 : [딥러닝 기본 실습/컨볼루션 신경망 모델 만들어보기](https://tykimos.github.io/Keras/2017/02/04/CNN_Getting_Started/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
