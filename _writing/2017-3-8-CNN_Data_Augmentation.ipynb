{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"컨볼루션 신경망 모델을 위한 데이터 부풀리기\"\n",
    "author: Taeyoung, Kim\n",
    "date:   2017-03-08 23:10:00\n",
    "categories: Lecture\n",
    "comments: true\n",
    "image: http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_combination.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 강좌에서는 컨볼루션 신경망 모델의 성능을 높이기 위한 방법 중 하나인 데이터 부풀리기에 대해서 알아보겠습니다. 훈련셋이 부족하거나 훈련셋이 시험셋의 특성을 충분히 반영하지 못할 때 이 방법을 사용하면 모델의 성능을 크게 향상시킬 수 있습니다. 케라스에서는 데이터 부풀리기를 위한 함수를 제공하기 때문에 파라미터 셋팅만으로 간단히 데이터 부풀리기를 할 수 있습니다.\n",
    "\n",
    "1. 현실적인 문제\n",
    "1. 기존 모델 결과보기\n",
    "1. 데이터 부풀리기\n",
    "1. 개선 모델 결과보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 현실적인 문제\n",
    "\n",
    "[컨볼루션 신경망 모델 만들어보기](https://tykimos.github.io/Keras/2017/03/08/CNN_Getting_Started/) 강좌에서 사용하였던 원, 사각형, 삼각형 데이터셋을 예제로 살펴보겠습니다. 구성은 훈련셋과 시험셋으로 되어 있는 데, 아래 그림은 훈련셋입니다.\n",
    "\n",
    "#### 훈련셋\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_1.png)\n",
    "\n",
    "그리고 아래 그림은 시험셋입니다. 훈련셋과 시험셋은 모두 한사람(제가) 그린 것이라 거의 비슷합니다. 그래서 그런지 100% 정확도의 좋은 결과를 얻었나 봅니다.\n",
    "\n",
    "#### 시험셋\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_2.png)\n",
    "\n",
    "100% 정확도를 얻은 모델이니 원, 사각형, 삼각형을 그려주면 다 분류를 해보겠다며 지인에게 자랑을 해봅니다. 그래서 지인이 그려준 시험셋은 다음과 같습니다.\n",
    "\n",
    "#### 도전 시험셋\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_3.png)\n",
    "\n",
    "막상 시험셋을 받아보니 자신감이 없어지면서 여러가지 생각이 듭니다.\n",
    "\n",
    "- 아, 이것도 원, 사각형, 삼각형이구나\n",
    "- 왜 이런 데이터를 진작에 학습시키지 않았을까?\n",
    "- 새로 받은 시험셋 일부를 학습시켜볼까?\n",
    "- 이렇게 간단한 문제도 개발과 현실과의 차이가 이렇게 나는데, 실제 문제는 더 상황이 좋지 않겠지?\n",
    "\n",
    "결국 하나의 결론에 도달합니다.\n",
    "\n",
    "    개발자가 시험셋을 만들면 안된다.\n",
    "\n",
    "하지만 어떠한 문제에서도 미래에 들어올 데이터에 대해서는 알 수가 없기 때문에, 비슷한 상황일 것 같습니다. 먼저 도전 시험셋으로 시험한 결과를 살펴본 뒤, 한정적인 훈련셋을 이용하여 최대한 발생할 수 있는 경우을 고려하여 훈련셋을 만드는 방법인 `데이터 부풀리기`에 대해서 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 기존 모델 결과보기\n",
    "\n",
    "[컨볼루션 신경망 모델 만들어보기](https://tykimos.github.io/Keras/2017/03/08/CNN_Getting_Started/) 강좌에서 사용했던 모델을 그대로 가지고 왔습니다. 제가 만든 시험셋에서는 결과가 100%나왔는데, 도전 시험셋으론 어떤 결과가 나오는 지 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n",
      "Epoch 1/200\n",
      "15/15 [==============================] - 0s - loss: 0.9365 - acc: 0.5778 - val_loss: 1.7078 - val_acc: 0.3333\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s - loss: 0.1786 - acc: 1.0000 - val_loss: 2.8848 - val_acc: 0.4000\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s - loss: 0.0233 - acc: 1.0000 - val_loss: 4.3072 - val_acc: 0.4000\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s - loss: 0.0148 - acc: 1.0000 - val_loss: 4.3684 - val_acc: 0.4000\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s - loss: 0.0338 - acc: 0.9778 - val_loss: 4.7764 - val_acc: 0.4000\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s - loss: 9.1348e-04 - acc: 1.0000 - val_loss: 4.7950 - val_acc: 0.4000\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s - loss: 3.7592e-04 - acc: 1.0000 - val_loss: 4.7583 - val_acc: 0.4000\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s - loss: 1.8625e-04 - acc: 1.0000 - val_loss: 5.1893 - val_acc: 0.4000\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s - loss: 9.8336e-05 - acc: 1.0000 - val_loss: 5.4292 - val_acc: 0.4000\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s - loss: 6.9932e-05 - acc: 1.0000 - val_loss: 5.6350 - val_acc: 0.4000\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s - loss: 5.1202e-05 - acc: 1.0000 - val_loss: 6.7341 - val_acc: 0.3333\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s - loss: 3.8069e-05 - acc: 1.0000 - val_loss: 6.8497 - val_acc: 0.4000\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s - loss: 2.9163e-05 - acc: 1.0000 - val_loss: 7.3107 - val_acc: 0.2000\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s - loss: 2.4330e-05 - acc: 1.0000 - val_loss: 4.2624 - val_acc: 0.5333\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s - loss: 1.9078e-05 - acc: 1.0000 - val_loss: 5.8984 - val_acc: 0.4667\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s - loss: 1.5593e-05 - acc: 1.0000 - val_loss: 5.4663 - val_acc: 0.4667\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-05 - acc: 1.0000 - val_loss: 6.8555 - val_acc: 0.4000\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s - loss: 1.0501e-05 - acc: 1.0000 - val_loss: 6.2624 - val_acc: 0.4667\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s - loss: 8.6985e-06 - acc: 1.0000 - val_loss: 6.6819 - val_acc: 0.3333\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s - loss: 7.3964e-06 - acc: 1.0000 - val_loss: 4.8042 - val_acc: 0.4667\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s - loss: 6.1566e-06 - acc: 1.0000 - val_loss: 6.4404 - val_acc: 0.4000\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s - loss: 5.1724e-06 - acc: 1.0000 - val_loss: 7.9978 - val_acc: 0.2667\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s - loss: 4.4386e-06 - acc: 1.0000 - val_loss: 5.8929 - val_acc: 0.4667\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s - loss: 3.8161e-06 - acc: 1.0000 - val_loss: 6.8087 - val_acc: 0.4000\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s - loss: 3.2889e-06 - acc: 1.0000 - val_loss: 6.5697 - val_acc: 0.4000\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s - loss: 2.9591e-06 - acc: 1.0000 - val_loss: 5.1263 - val_acc: 0.4667\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s - loss: 2.6372e-06 - acc: 1.0000 - val_loss: 7.6977 - val_acc: 0.4000\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s - loss: 2.3604e-06 - acc: 1.0000 - val_loss: 5.9260 - val_acc: 0.4667\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s - loss: 2.1895e-06 - acc: 1.0000 - val_loss: 6.0704 - val_acc: 0.4667\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s - loss: 1.9272e-06 - acc: 1.0000 - val_loss: 7.1156 - val_acc: 0.3333\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s - loss: 1.7762e-06 - acc: 1.0000 - val_loss: 5.0830 - val_acc: 0.4000\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s - loss: 1.6160e-06 - acc: 1.0000 - val_loss: 6.2688 - val_acc: 0.4667\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s - loss: 1.4769e-06 - acc: 1.0000 - val_loss: 5.9950 - val_acc: 0.4000\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s - loss: 1.3775e-06 - acc: 1.0000 - val_loss: 4.3484 - val_acc: 0.5333\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s - loss: 1.2729e-06 - acc: 1.0000 - val_loss: 4.9514 - val_acc: 0.4667\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s - loss: 1.1789e-06 - acc: 1.0000 - val_loss: 6.9151 - val_acc: 0.4000\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s - loss: 1.1166e-06 - acc: 1.0000 - val_loss: 6.9353 - val_acc: 0.4000\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s - loss: 1.0199e-06 - acc: 1.0000 - val_loss: 6.9532 - val_acc: 0.4000\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s - loss: 9.6030e-07 - acc: 1.0000 - val_loss: 6.9636 - val_acc: 0.4000\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s - loss: 9.0202e-07 - acc: 1.0000 - val_loss: 6.9849 - val_acc: 0.4000\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s - loss: 8.5698e-07 - acc: 1.0000 - val_loss: 7.0064 - val_acc: 0.4000\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s - loss: 8.0798e-07 - acc: 1.0000 - val_loss: 7.0224 - val_acc: 0.4000\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s - loss: 7.6559e-07 - acc: 1.0000 - val_loss: 6.3018 - val_acc: 0.4667\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s - loss: 7.2983e-07 - acc: 1.0000 - val_loss: 5.6684 - val_acc: 0.5333\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s - loss: 6.9274e-07 - acc: 1.0000 - val_loss: 8.0276 - val_acc: 0.2667\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s - loss: 6.5963e-07 - acc: 1.0000 - val_loss: 7.1240 - val_acc: 0.3333\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s - loss: 6.2784e-07 - acc: 1.0000 - val_loss: 8.4858 - val_acc: 0.3333\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s - loss: 5.9870e-07 - acc: 1.0000 - val_loss: 8.0773 - val_acc: 0.4000\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s - loss: 5.7750e-07 - acc: 1.0000 - val_loss: 4.4090 - val_acc: 0.6000\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s - loss: 5.5101e-07 - acc: 1.0000 - val_loss: 6.5994 - val_acc: 0.4000\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s - loss: 5.2187e-07 - acc: 1.0000 - val_loss: 6.1428 - val_acc: 0.5333\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s - loss: 5.0200e-07 - acc: 1.0000 - val_loss: 7.7013 - val_acc: 0.4000\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s - loss: 4.8876e-07 - acc: 1.0000 - val_loss: 7.8915 - val_acc: 0.4000\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s - loss: 4.7021e-07 - acc: 1.0000 - val_loss: 7.3177 - val_acc: 0.3333\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s - loss: 4.4902e-07 - acc: 1.0000 - val_loss: 8.1886 - val_acc: 0.4000\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s - loss: 4.3180e-07 - acc: 1.0000 - val_loss: 8.7035 - val_acc: 0.2667\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s - loss: 4.1988e-07 - acc: 1.0000 - val_loss: 9.9866 - val_acc: 0.2667\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 0s - loss: 4.0266e-07 - acc: 1.0000 - val_loss: 6.6888 - val_acc: 0.4000\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s - loss: 3.8942e-07 - acc: 1.0000 - val_loss: 8.7948 - val_acc: 0.3333\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s - loss: 3.8412e-07 - acc: 1.0000 - val_loss: 7.5903 - val_acc: 0.4000\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s - loss: 3.7220e-07 - acc: 1.0000 - val_loss: 8.4657 - val_acc: 0.3333\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s - loss: 3.5763e-07 - acc: 1.0000 - val_loss: 7.6254 - val_acc: 0.3333\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s - loss: 3.4703e-07 - acc: 1.0000 - val_loss: 5.3983 - val_acc: 0.4667\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s - loss: 3.4041e-07 - acc: 1.0000 - val_loss: 7.2354 - val_acc: 0.3333\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s - loss: 3.2716e-07 - acc: 1.0000 - val_loss: 7.7281 - val_acc: 0.4000\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s - loss: 3.2054e-07 - acc: 1.0000 - val_loss: 6.2934 - val_acc: 0.5333\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s - loss: 3.1259e-07 - acc: 1.0000 - val_loss: 4.8062 - val_acc: 0.5333\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s - loss: 3.0067e-07 - acc: 1.0000 - val_loss: 7.8008 - val_acc: 0.4000\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s - loss: 2.9802e-07 - acc: 1.0000 - val_loss: 7.2822 - val_acc: 0.4000\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s - loss: 2.9273e-07 - acc: 1.0000 - val_loss: 7.3744 - val_acc: 0.4000\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s - loss: 2.8610e-07 - acc: 1.0000 - val_loss: 8.8273 - val_acc: 0.2667\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s - loss: 2.7683e-07 - acc: 1.0000 - val_loss: 8.6334 - val_acc: 0.3333\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s - loss: 2.7021e-07 - acc: 1.0000 - val_loss: 7.9112 - val_acc: 0.4000\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s - loss: 2.6491e-07 - acc: 1.0000 - val_loss: 7.3863 - val_acc: 0.4667\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s - loss: 2.6094e-07 - acc: 1.0000 - val_loss: 7.7653 - val_acc: 0.3333\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s - loss: 2.5299e-07 - acc: 1.0000 - val_loss: 7.3229 - val_acc: 0.4000\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s - loss: 2.5034e-07 - acc: 1.0000 - val_loss: 7.3281 - val_acc: 0.4000\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s - loss: 2.4372e-07 - acc: 1.0000 - val_loss: 7.4755 - val_acc: 0.4000\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s - loss: 2.3842e-07 - acc: 1.0000 - val_loss: 7.6145 - val_acc: 0.3333\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s - loss: 2.3445e-07 - acc: 1.0000 - val_loss: 7.6994 - val_acc: 0.3333\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s - loss: 2.3180e-07 - acc: 1.0000 - val_loss: 9.2193 - val_acc: 0.2667\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s - loss: 2.2650e-07 - acc: 1.0000 - val_loss: 6.8826 - val_acc: 0.4000\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s - loss: 2.2252e-07 - acc: 1.0000 - val_loss: 8.1517 - val_acc: 0.3333\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s - loss: 2.1855e-07 - acc: 1.0000 - val_loss: 5.4855 - val_acc: 0.5333\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s - loss: 2.1325e-07 - acc: 1.0000 - val_loss: 6.2673 - val_acc: 0.5333\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s - loss: 2.0928e-07 - acc: 1.0000 - val_loss: 6.0546 - val_acc: 0.4667\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s - loss: 2.0928e-07 - acc: 1.0000 - val_loss: 5.5492 - val_acc: 0.5333\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s - loss: 2.0530e-07 - acc: 1.0000 - val_loss: 6.1610 - val_acc: 0.4667\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s - loss: 2.0398e-07 - acc: 1.0000 - val_loss: 7.7695 - val_acc: 0.4000\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s - loss: 2.0001e-07 - acc: 1.0000 - val_loss: 5.7709 - val_acc: 0.5333\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s - loss: 1.9736e-07 - acc: 1.0000 - val_loss: 6.4421 - val_acc: 0.4667\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s - loss: 1.9603e-07 - acc: 1.0000 - val_loss: 5.9871 - val_acc: 0.4667\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s - loss: 1.9338e-07 - acc: 1.0000 - val_loss: 8.6278 - val_acc: 0.2667\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 0s - loss: 1.8941e-07 - acc: 1.0000 - val_loss: 6.7015 - val_acc: 0.4000\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 0s - loss: 1.8941e-07 - acc: 1.0000 - val_loss: 8.9728 - val_acc: 0.2667\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 0s - loss: 1.8676e-07 - acc: 1.0000 - val_loss: 8.0798 - val_acc: 0.3333\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 0s - loss: 1.8544e-07 - acc: 1.0000 - val_loss: 5.5690 - val_acc: 0.4667\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 0s - loss: 1.8279e-07 - acc: 1.0000 - val_loss: 8.0614 - val_acc: 0.4000\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 0s - loss: 1.8146e-07 - acc: 1.0000 - val_loss: 8.3042 - val_acc: 0.3333\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 0s - loss: 1.7881e-07 - acc: 1.0000 - val_loss: 4.1344 - val_acc: 0.6000\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 0s - loss: 1.7484e-07 - acc: 1.0000 - val_loss: 6.4706 - val_acc: 0.4000\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 0s - loss: 1.7352e-07 - acc: 1.0000 - val_loss: 8.5244 - val_acc: 0.4000\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 0s - loss: 1.7352e-07 - acc: 1.0000 - val_loss: 8.0068 - val_acc: 0.4000\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 0s - loss: 1.7219e-07 - acc: 1.0000 - val_loss: 5.4048 - val_acc: 0.4667\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 0s - loss: 1.7087e-07 - acc: 1.0000 - val_loss: 7.9656 - val_acc: 0.4000\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 0s - loss: 1.6954e-07 - acc: 1.0000 - val_loss: 7.8933 - val_acc: 0.3333\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 0s - loss: 1.6689e-07 - acc: 1.0000 - val_loss: 7.7049 - val_acc: 0.4000\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 0s - loss: 1.6424e-07 - acc: 1.0000 - val_loss: 5.7357 - val_acc: 0.4667\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 0s - loss: 1.6292e-07 - acc: 1.0000 - val_loss: 6.1875 - val_acc: 0.5333\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 0s - loss: 1.6292e-07 - acc: 1.0000 - val_loss: 8.5991 - val_acc: 0.3333\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 0s - loss: 1.6159e-07 - acc: 1.0000 - val_loss: 7.0699 - val_acc: 0.4667\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 0s - loss: 1.6027e-07 - acc: 1.0000 - val_loss: 6.0678 - val_acc: 0.5333\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 0s - loss: 1.5762e-07 - acc: 1.0000 - val_loss: 5.9487 - val_acc: 0.5333\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 0s - loss: 1.5762e-07 - acc: 1.0000 - val_loss: 7.6626 - val_acc: 0.4000\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 0s - loss: 1.5497e-07 - acc: 1.0000 - val_loss: 5.4419 - val_acc: 0.6000\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 0s - loss: 1.5497e-07 - acc: 1.0000 - val_loss: 7.2713 - val_acc: 0.4667\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 0s - loss: 1.5497e-07 - acc: 1.0000 - val_loss: 7.3382 - val_acc: 0.4000\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 0s - loss: 1.5232e-07 - acc: 1.0000 - val_loss: 6.6540 - val_acc: 0.4000\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 0s - loss: 1.5232e-07 - acc: 1.0000 - val_loss: 7.5931 - val_acc: 0.4667\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 0s - loss: 1.5100e-07 - acc: 1.0000 - val_loss: 7.6853 - val_acc: 0.3333\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 0s - loss: 1.5100e-07 - acc: 1.0000 - val_loss: 7.4277 - val_acc: 0.4000\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 0s - loss: 1.5100e-07 - acc: 1.0000 - val_loss: 7.4924 - val_acc: 0.4000\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 0s - loss: 1.4835e-07 - acc: 1.0000 - val_loss: 7.6761 - val_acc: 0.4000\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 0s - loss: 1.4967e-07 - acc: 1.0000 - val_loss: 6.1006 - val_acc: 0.5333\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 0s - loss: 1.4570e-07 - acc: 1.0000 - val_loss: 6.8662 - val_acc: 0.4667\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 0s - loss: 1.4702e-07 - acc: 1.0000 - val_loss: 8.0580 - val_acc: 0.4000\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 0s - loss: 1.4570e-07 - acc: 1.0000 - val_loss: 7.5025 - val_acc: 0.4000\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 0s - loss: 1.4570e-07 - acc: 1.0000 - val_loss: 6.8848 - val_acc: 0.4000\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 0s - loss: 1.4438e-07 - acc: 1.0000 - val_loss: 9.6512 - val_acc: 0.2667\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 0s - loss: 1.4438e-07 - acc: 1.0000 - val_loss: 6.8806 - val_acc: 0.4000\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 0s - loss: 1.4305e-07 - acc: 1.0000 - val_loss: 7.5080 - val_acc: 0.4000\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 0s - loss: 1.4305e-07 - acc: 1.0000 - val_loss: 7.5884 - val_acc: 0.3333\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 0s - loss: 1.4305e-07 - acc: 1.0000 - val_loss: 7.9502 - val_acc: 0.4000\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 0s - loss: 1.4305e-07 - acc: 1.0000 - val_loss: 8.8953 - val_acc: 0.3333\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 0s - loss: 1.4305e-07 - acc: 1.0000 - val_loss: 6.5040 - val_acc: 0.4667\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 0s - loss: 1.4040e-07 - acc: 1.0000 - val_loss: 7.6391 - val_acc: 0.4000\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 0s - loss: 1.3775e-07 - acc: 1.0000 - val_loss: 7.8212 - val_acc: 0.4000\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 6.1266 - val_acc: 0.4667\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 8.2149 - val_acc: 0.3333\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 6.4383 - val_acc: 0.4667\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 6.2626 - val_acc: 0.4667\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 7.5705 - val_acc: 0.4000\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 8.5838 - val_acc: 0.3333\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 8.5818 - val_acc: 0.3333\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 5.6728 - val_acc: 0.5333\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 6.1981 - val_acc: 0.4667\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 0s - loss: 1.3643e-07 - acc: 1.0000 - val_loss: 9.1403 - val_acc: 0.3333\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 0s - loss: 1.3510e-07 - acc: 1.0000 - val_loss: 7.0678 - val_acc: 0.4000\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 0s - loss: 1.3510e-07 - acc: 1.0000 - val_loss: 6.9610 - val_acc: 0.4000\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 0s - loss: 1.3378e-07 - acc: 1.0000 - val_loss: 6.9904 - val_acc: 0.4667\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 0s - loss: 1.3245e-07 - acc: 1.0000 - val_loss: 8.5876 - val_acc: 0.3333\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 0s - loss: 1.3245e-07 - acc: 1.0000 - val_loss: 6.7934 - val_acc: 0.4000\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 0s - loss: 1.3245e-07 - acc: 1.0000 - val_loss: 7.7138 - val_acc: 0.4000\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 0s - loss: 1.3245e-07 - acc: 1.0000 - val_loss: 7.2516 - val_acc: 0.4667\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 0s - loss: 1.3245e-07 - acc: 1.0000 - val_loss: 6.7505 - val_acc: 0.4667\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 0s - loss: 1.3245e-07 - acc: 1.0000 - val_loss: 5.9007 - val_acc: 0.4667\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 0s - loss: 1.3113e-07 - acc: 1.0000 - val_loss: 6.6651 - val_acc: 0.4000\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 0s - loss: 1.3113e-07 - acc: 1.0000 - val_loss: 8.0280 - val_acc: 0.3333\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 0s - loss: 1.3113e-07 - acc: 1.0000 - val_loss: 9.1607 - val_acc: 0.2667\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 0s - loss: 1.2981e-07 - acc: 1.0000 - val_loss: 10.8069 - val_acc: 0.2000\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 0s - loss: 1.2981e-07 - acc: 1.0000 - val_loss: 9.7629 - val_acc: 0.2667\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 0s - loss: 1.2981e-07 - acc: 1.0000 - val_loss: 7.1924 - val_acc: 0.4000\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 0s - loss: 1.2981e-07 - acc: 1.0000 - val_loss: 7.9832 - val_acc: 0.4000\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 0s - loss: 1.2981e-07 - acc: 1.0000 - val_loss: 5.7538 - val_acc: 0.5333\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 8.7365 - val_acc: 0.3333\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 9.0983 - val_acc: 0.2667\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 8.8128 - val_acc: 0.3333\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 8.6635 - val_acc: 0.4000\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 9.0941 - val_acc: 0.2667\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 7.8956 - val_acc: 0.4000\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 9.3937 - val_acc: 0.2667\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 0s - loss: 1.2848e-07 - acc: 1.0000 - val_loss: 8.4029 - val_acc: 0.3333\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 0s - loss: 1.2716e-07 - acc: 1.0000 - val_loss: 5.0366 - val_acc: 0.5333\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 0s - loss: 1.2716e-07 - acc: 1.0000 - val_loss: 9.4677 - val_acc: 0.2667\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 0s - loss: 1.2583e-07 - acc: 1.0000 - val_loss: 7.1388 - val_acc: 0.4000\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 0s - loss: 1.2583e-07 - acc: 1.0000 - val_loss: 7.9562 - val_acc: 0.4000\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 0s - loss: 1.2583e-07 - acc: 1.0000 - val_loss: 8.4538 - val_acc: 0.3333\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 0s - loss: 1.2583e-07 - acc: 1.0000 - val_loss: 6.5135 - val_acc: 0.4667\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 0s - loss: 1.2583e-07 - acc: 1.0000 - val_loss: 7.5547 - val_acc: 0.4000\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 6.3378 - val_acc: 0.4667\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 6.2140 - val_acc: 0.4667\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 8.5726 - val_acc: 0.3333\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 4.8808 - val_acc: 0.6000\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 8.1219 - val_acc: 0.3333\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 8.7693 - val_acc: 0.2667\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 7.4999 - val_acc: 0.4000\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 7.8309 - val_acc: 0.3333\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 0s - loss: 1.2451e-07 - acc: 1.0000 - val_loss: 8.9592 - val_acc: 0.2667\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 0s - loss: 1.2318e-07 - acc: 1.0000 - val_loss: 4.2872 - val_acc: 0.5333\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 0s - loss: 1.2318e-07 - acc: 1.0000 - val_loss: 8.4822 - val_acc: 0.3333\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 0s - loss: 1.2318e-07 - acc: 1.0000 - val_loss: 7.6125 - val_acc: 0.4000\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 0s - loss: 1.2318e-07 - acc: 1.0000 - val_loss: 6.7947 - val_acc: 0.4667\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 0s - loss: 1.2318e-07 - acc: 1.0000 - val_loss: 6.9328 - val_acc: 0.4000\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 7.1376 - val_acc: 0.4000\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 6.4387 - val_acc: 0.4000\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 6.2569 - val_acc: 0.4667\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 7.4963 - val_acc: 0.4000\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 7.4959 - val_acc: 0.4000\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 7.4956 - val_acc: 0.4000\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 0s - loss: 1.2186e-07 - acc: 1.0000 - val_loss: 7.4950 - val_acc: 0.4000\n",
      "-- Evaluate --\n",
      "acc: 40.00%\n",
      "-- Predict --\n",
      "[[0.000 0.013 0.987]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.727 0.001 0.272]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.013 0.987]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(3)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/train',\n",
    "        target_size=(24, 24),\n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/test',\n",
    "        target_size=(24, 24),    \n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(24,24,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습시키기\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=15,\n",
    "        epochs=200,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=5)\n",
    "\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            test_generator, \n",
    "            steps = 5)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "\n",
    "output = model.predict_generator(\n",
    "            test_generator, \n",
    "            steps = 5)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수행결과는 40%입니다. 세개 중 하나 찍는 문제인데도 불구하고 50%로 못 넘깁니다. 오버피팅이 제대로 된 모델이라고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 데이터 부풀리기\n",
    "\n",
    "케라스에서는 `ImageDataGenerator` 함수를 통해서 데이터 부풀리기 기능을 제공합니다. [keras.io](https://keras.io/preprocessing/image/#imagedatagenerator) 페이지를 보면, 아래와 같은 옵션으로 데이터 부풀리기를 할 수 있습니다.\n",
    "\n",
    "    keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0.,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None,\n",
    "    data_format=K.image_data_format())\n",
    "\n",
    "그럼 훈련셋 중 하나인 삼각형을 골라 데이터 부풀리기를 해보겠습니다. 원본이 되는 삼각형은 다음과 같습니다.\n",
    "\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_4.png)\n",
    "\n",
    "이 삼각형을 ImageDataGenerator 함수을 이용하여 각 파라미터별로 어떻게 부풀리기를 하는 지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rotation_range = 90\n",
    "지정된 각도 범위내에서 임의로 원본이미지를 회전시킵니다. 단위는 도이며, 정수형입니다. 예를 들어 90이라면 0도에서 90도 사이에 임의의 각도로 회전시킵니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_rotate.png)\n",
    "                                   \n",
    "#### width_shift_range = 0.1\n",
    "지정된 수평방향 이동 범위내에서 임의로 원본이미지를 이동시킵니다. 수치는 전체 넓이의 비율(실수)로 나타냅니다. 예를 들어 0.1이고 전체 넓이가 100이면, 10픽셀 내외로 좌우 이동시킵니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_width_shift.png)\n",
    "\n",
    "#### height_shift_range = 0.1\n",
    "지정된 수직방향 이동 범위내에서 임의로 원본이미지를 이동시킵니다. 수치는 전체 높이의 비율(실수)로 나타냅니다. 예를 들어 0.1이고 전체 높이가 100이면, 10픽셀 내외로 상하 이동시킵니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_height_shift.png)\n",
    "\n",
    "#### shear_range = 0.5\n",
    "밀림 강도 범위내에서 임의로 원본이미지를 변형시킵니다. 수치는 시계반대방향으로 밀림 강도를 라디안으로 나타냅니다. 예를 들어 0.5이라면, 0.5 라이안내외로 시계반대방향으로 변형시킵니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_shear.png)\n",
    "\n",
    "#### zoom_range = 0.3\n",
    "지정된 확대/축소 범위내에서 임의로 원본이미지를 확대/축소합니다. \"1-수치\"부터 \"1+수치\"사이 범위로 확대/축소를 합니다. 예를 들어 0.3이라면, 0.7배에서 1.3배 크기 변화를 시킵니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_zoom.png)\n",
    "\n",
    "#### horizontal_flip = True\n",
    "수평방향으로 뒤집기를 합니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_horizontal_flip.png)\n",
    "\n",
    "#### vertical_flip = True\n",
    "수직방향으로 뒤집기를 합니다.\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_vertical_flip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 ImageDataGenerator함수를 이용하여 지정된 파라미터로 원본이미지에 대해 데이터 부풀리기를 수행한 후 그 결과를 특정 폴더에 저장하는 코드입니다. 여러 파라미터를 사용하였기 때문에 이를 혼합하여 데이터 부풀리기를 수행합니다. 즉 확대/축소도 하고 좌우 이동도 지정하였다면, 축소하면서 좌로 이동된 이미지도 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(5)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "data_aug_gen = ImageDataGenerator(rescale=1./255, \n",
    "                                  rotation_range=15,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  shear_range=0.5,\n",
    "                                  zoom_range=[0.8, 2.0],\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "                                   \n",
    "img = load_img('warehouse/hard_handwriting_shape/train/triangle/triangle001.png')\n",
    "x = img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "i = 0\n",
    "\n",
    "# 이 for는 무한으로 반복되기 때문에 우리가 원하는 반복횟수를 지정하여, 지정된 반복횟수가 되면 빠져나오도록 해야합니다.\n",
    "for batch in train_datagen.flow(x, batch_size=1, save_to_dir='warehouse/preview', save_prefix='tri', save_format='png'):\n",
    "    i += 1\n",
    "    if i > 30: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드로 데이터 부풀리기가 수행된 결과 이미지는 다음과 같습니다. 지인이 만든 도전 시험셋 중 비슷한 것들도 보입니다.\n",
    "\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_5_combination.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 개선 모델 결과보기\n",
    "\n",
    "데이터 부풀리기를 하기 위해서는 기존 코드에서 아래 코드를 추가합니다. 각 파라미터 설정 값에 따라 결과가 다르기 나오니, 실제 데이터에 있을만한 수준으로 적정값을 지정하셔야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.5,\n",
    "                                   zoom_range=[0.8, 2.0],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정된 전체 코드는 다음과 같습니다. 참고로 시험셋은 데이터 부풀리기를 할 필요가 없으니, test_datagen 객체 생성 시에는 별도의 파라미터를 추가하지 않았습니다. 그리고 fit_generator함수에서 steps_per_epoch의 값은 기존 15개에서 더 많은 수 (현재 예는 1500개)로 설정합니다. batch_size * steps_per_epoch가 전체 샘플 수 인데, 데이터 부풀리기를 하지 않을 때는 기존의 15개의 배치사이즈(3개)로 전체 45개를 모두 학습에 사용할 수 있지만, ImageDataGenerator함수를 통해 데이터 부풀리기는 할 때는 하나의 샘플로 여러 개의 결과를 얻기 때문에 요청하는 데로 무한의 샘플이 제공됩니다. 여기서는 100배 정도인 1500개로 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n",
      "Epoch 1/200\n",
      "1500/1500 [==============================] - 31s - loss: 0.4396 - acc: 0.8109 - val_loss: 2.0550 - val_acc: 0.6000\n",
      "Epoch 2/200\n",
      "1500/1500 [==============================] - 41s - loss: 0.1653 - acc: 0.9427 - val_loss: 1.7761 - val_acc: 0.8000\n",
      "Epoch 3/200\n",
      "1500/1500 [==============================] - 42s - loss: 0.1155 - acc: 0.9609 - val_loss: 2.5279 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "1500/1500 [==============================] - 43s - loss: 0.0958 - acc: 0.9689 - val_loss: 2.7588 - val_acc: 0.5333\n",
      "Epoch 5/200\n",
      "1500/1500 [==============================] - 44s - loss: 0.0624 - acc: 0.9789 - val_loss: 3.4055 - val_acc: 0.6000\n",
      "Epoch 6/200\n",
      "1500/1500 [==============================] - 44s - loss: 0.0548 - acc: 0.9829 - val_loss: 2.6890 - val_acc: 0.6667\n",
      "Epoch 7/200\n",
      "1500/1500 [==============================] - 45s - loss: 0.0503 - acc: 0.9807 - val_loss: 1.9911 - val_acc: 0.7333\n",
      "Epoch 8/200\n",
      "1500/1500 [==============================] - 46s - loss: 0.0472 - acc: 0.9844 - val_loss: 3.4829 - val_acc: 0.7333\n",
      "Epoch 9/200\n",
      "1500/1500 [==============================] - 46s - loss: 0.0455 - acc: 0.9838 - val_loss: 2.0811 - val_acc: 0.6000\n",
      "Epoch 10/200\n",
      "1500/1500 [==============================] - 47s - loss: 0.0393 - acc: 0.9873 - val_loss: 2.4801 - val_acc: 0.7333\n",
      "Epoch 11/200\n",
      "1500/1500 [==============================] - 48s - loss: 0.0445 - acc: 0.9840 - val_loss: 2.4721 - val_acc: 0.6667\n",
      "Epoch 12/200\n",
      "1500/1500 [==============================] - 48s - loss: 0.0451 - acc: 0.9856 - val_loss: 3.0109 - val_acc: 0.7333\n",
      "Epoch 13/200\n",
      "1500/1500 [==============================] - 47s - loss: 0.0374 - acc: 0.9893 - val_loss: 2.4476 - val_acc: 0.7333\n",
      "Epoch 14/200\n",
      "1500/1500 [==============================] - 49s - loss: 0.0358 - acc: 0.9900 - val_loss: 2.3594 - val_acc: 0.7333\n",
      "Epoch 15/200\n",
      "1500/1500 [==============================] - 47s - loss: 0.0423 - acc: 0.9880 - val_loss: 1.2934 - val_acc: 0.8000\n",
      "Epoch 16/200\n",
      "1500/1500 [==============================] - 48s - loss: 0.0208 - acc: 0.9947 - val_loss: 2.6688 - val_acc: 0.7333\n",
      "Epoch 17/200\n",
      "1500/1500 [==============================] - 48s - loss: 0.0373 - acc: 0.9887 - val_loss: 2.6695 - val_acc: 0.7333\n",
      "Epoch 18/200\n",
      "1500/1500 [==============================] - 47s - loss: 0.0376 - acc: 0.9896 - val_loss: 1.9232 - val_acc: 0.8000\n",
      "Epoch 19/200\n",
      "1500/1500 [==============================] - 46s - loss: 0.0328 - acc: 0.9900 - val_loss: 2.2388 - val_acc: 0.7333\n",
      "Epoch 20/200\n",
      "1500/1500 [==============================] - 46s - loss: 0.0146 - acc: 0.9944 - val_loss: 1.9505 - val_acc: 0.7333\n",
      "Epoch 21/200\n",
      "1500/1500 [==============================] - 46s - loss: 0.0308 - acc: 0.9898 - val_loss: 2.0749 - val_acc: 0.7333\n",
      "Epoch 22/200\n",
      "1500/1500 [==============================] - 46s - loss: 0.0292 - acc: 0.9936 - val_loss: 2.3499 - val_acc: 0.7333\n",
      "Epoch 23/200\n",
      "1500/1500 [==============================] - 48s - loss: 0.0280 - acc: 0.9916 - val_loss: 3.0220 - val_acc: 0.7333\n",
      "Epoch 24/200\n",
      "1500/1500 [==============================] - 48s - loss: 0.0320 - acc: 0.9902 - val_loss: 1.9710 - val_acc: 0.8000\n",
      "Epoch 25/200\n",
      "1500/1500 [==============================] - 51s - loss: 0.0289 - acc: 0.9913 - val_loss: 1.7652 - val_acc: 0.7333\n",
      "Epoch 26/200\n",
      "1500/1500 [==============================] - 66s - loss: 0.0190 - acc: 0.9953 - val_loss: 2.1829 - val_acc: 0.7333\n",
      "Epoch 27/200\n",
      "1500/1500 [==============================] - 82s - loss: 0.0296 - acc: 0.9929 - val_loss: 2.9217 - val_acc: 0.7333\n",
      "Epoch 28/200\n",
      "1500/1500 [==============================] - 55s - loss: 0.0270 - acc: 0.9911 - val_loss: 1.7723 - val_acc: 0.8000\n",
      "Epoch 29/200\n",
      "1500/1500 [==============================] - 54s - loss: 0.0374 - acc: 0.9909 - val_loss: 2.3548 - val_acc: 0.7333\n",
      "Epoch 30/200\n",
      "1500/1500 [==============================] - 54s - loss: 0.0246 - acc: 0.9942 - val_loss: 1.7970 - val_acc: 0.8000\n",
      "Epoch 31/200\n",
      "1500/1500 [==============================] - 54s - loss: 0.0421 - acc: 0.9900 - val_loss: 4.1466 - val_acc: 0.6000\n",
      "Epoch 32/200\n",
      "1500/1500 [==============================] - 54s - loss: 0.0269 - acc: 0.9911 - val_loss: 3.0288 - val_acc: 0.7333\n",
      "Epoch 33/200\n",
      "1500/1500 [==============================] - 54s - loss: 0.0219 - acc: 0.9933 - val_loss: 2.8926 - val_acc: 0.7333\n",
      "Epoch 34/200\n",
      "1500/1500 [==============================] - 54s - loss: 0.0222 - acc: 0.9933 - val_loss: 2.5987 - val_acc: 0.6667\n",
      "Epoch 35/200\n",
      "1500/1500 [==============================] - 55s - loss: 0.0181 - acc: 0.9951 - val_loss: 2.9477 - val_acc: 0.7333\n",
      "Epoch 36/200\n",
      "1500/1500 [==============================] - 57s - loss: 0.0350 - acc: 0.9909 - val_loss: 2.8236 - val_acc: 0.7333\n",
      "Epoch 37/200\n",
      "1500/1500 [==============================] - 59s - loss: 0.0164 - acc: 0.9947 - val_loss: 3.2707 - val_acc: 0.7333\n",
      "Epoch 38/200\n",
      "1500/1500 [==============================] - 58s - loss: 0.0251 - acc: 0.9940 - val_loss: 2.2870 - val_acc: 0.6667\n",
      "Epoch 39/200\n",
      "1500/1500 [==============================] - 56s - loss: 0.0274 - acc: 0.9924 - val_loss: 3.8786 - val_acc: 0.7333\n",
      "Epoch 40/200\n",
      "1500/1500 [==============================] - 58s - loss: 0.0189 - acc: 0.9949 - val_loss: 2.0784 - val_acc: 0.8000\n",
      "Epoch 41/200\n",
      "1500/1500 [==============================] - 58s - loss: 0.0201 - acc: 0.9936 - val_loss: 2.5132 - val_acc: 0.7333\n",
      "Epoch 42/200\n",
      "1500/1500 [==============================] - 59s - loss: 0.0219 - acc: 0.9942 - val_loss: 2.7321 - val_acc: 0.7333\n",
      "Epoch 43/200\n",
      "1500/1500 [==============================] - 62s - loss: 0.0233 - acc: 0.9936 - val_loss: 3.1450 - val_acc: 0.7333\n",
      "Epoch 44/200\n",
      "1500/1500 [==============================] - 63s - loss: 0.0233 - acc: 0.9944 - val_loss: 2.3506 - val_acc: 0.8000\n",
      "Epoch 45/200\n",
      "1500/1500 [==============================] - 63s - loss: 0.0213 - acc: 0.9940 - val_loss: 3.0121 - val_acc: 0.7333\n",
      "Epoch 46/200\n",
      "1500/1500 [==============================] - 65s - loss: 0.0271 - acc: 0.9933 - val_loss: 2.1710 - val_acc: 0.8667\n",
      "Epoch 47/200\n",
      "1500/1500 [==============================] - 70s - loss: 0.0218 - acc: 0.9953 - val_loss: 4.1917 - val_acc: 0.6000\n",
      "Epoch 48/200\n",
      "1500/1500 [==============================] - 74s - loss: 0.0253 - acc: 0.9942 - val_loss: 5.2972 - val_acc: 0.6000\n",
      "Epoch 49/200\n",
      "1500/1500 [==============================] - 76s - loss: 0.0290 - acc: 0.9951 - val_loss: 2.5908 - val_acc: 0.7333\n",
      "Epoch 50/200\n",
      "1500/1500 [==============================] - 78s - loss: 0.0395 - acc: 0.9913 - val_loss: 3.9308 - val_acc: 0.7333\n",
      "Epoch 51/200\n",
      "1500/1500 [==============================] - 79s - loss: 0.0254 - acc: 0.9933 - val_loss: 3.2239 - val_acc: 0.7333\n",
      "Epoch 52/200\n",
      "1500/1500 [==============================] - 82s - loss: 0.0305 - acc: 0.9918 - val_loss: 4.3126 - val_acc: 0.6667\n",
      "Epoch 53/200\n",
      "1500/1500 [==============================] - 75s - loss: 0.0230 - acc: 0.9938 - val_loss: 2.7966 - val_acc: 0.7333\n",
      "Epoch 54/200\n",
      "1500/1500 [==============================] - 75s - loss: 0.0263 - acc: 0.9947 - val_loss: 2.2907 - val_acc: 0.8000\n",
      "Epoch 55/200\n",
      "1500/1500 [==============================] - 75s - loss: 0.0275 - acc: 0.9927 - val_loss: 1.4042 - val_acc: 0.8000\n",
      "Epoch 56/200\n",
      "1500/1500 [==============================] - 73s - loss: 0.0213 - acc: 0.9944 - val_loss: 2.4531 - val_acc: 0.7333\n",
      "Epoch 57/200\n",
      "1500/1500 [==============================] - 77s - loss: 0.0263 - acc: 0.9936 - val_loss: 2.6409 - val_acc: 0.8000\n",
      "Epoch 58/200\n",
      "1500/1500 [==============================] - 78s - loss: 0.0315 - acc: 0.9938 - val_loss: 4.5275 - val_acc: 0.6000\n",
      "Epoch 59/200\n",
      "1500/1500 [==============================] - 79s - loss: 0.0200 - acc: 0.9940 - val_loss: 4.1524 - val_acc: 0.6667\n",
      "Epoch 60/200\n",
      "1500/1500 [==============================] - 81s - loss: 0.0139 - acc: 0.9964 - val_loss: 4.3205 - val_acc: 0.7333\n",
      "Epoch 61/200\n",
      "1500/1500 [==============================] - 81s - loss: 0.0248 - acc: 0.9942 - val_loss: 0.9571 - val_acc: 0.8667\n",
      "Epoch 62/200\n",
      "1500/1500 [==============================] - 79s - loss: 0.0219 - acc: 0.9940 - val_loss: 1.8518 - val_acc: 0.7333\n",
      "Epoch 63/200\n",
      "1500/1500 [==============================] - 78s - loss: 0.0245 - acc: 0.9933 - val_loss: 2.0832 - val_acc: 0.7333\n",
      "Epoch 64/200\n",
      "1500/1500 [==============================] - 79s - loss: 0.0207 - acc: 0.9947 - val_loss: 1.9623 - val_acc: 0.8000\n",
      "Epoch 65/200\n",
      "1500/1500 [==============================] - 83s - loss: 0.0339 - acc: 0.9911 - val_loss: 2.6380 - val_acc: 0.7333\n",
      "Epoch 66/200\n",
      "1500/1500 [==============================] - 81s - loss: 0.0135 - acc: 0.9960 - val_loss: 3.3388 - val_acc: 0.7333\n",
      "Epoch 67/200\n",
      "1500/1500 [==============================] - 79s - loss: 0.0163 - acc: 0.9958 - val_loss: 3.1180 - val_acc: 0.7333\n",
      "Epoch 68/200\n",
      "1500/1500 [==============================] - 78s - loss: 0.0287 - acc: 0.9953 - val_loss: 5.7074 - val_acc: 0.6000\n",
      "Epoch 69/200\n",
      "1500/1500 [==============================] - 79s - loss: 0.0209 - acc: 0.9951 - val_loss: 3.3110 - val_acc: 0.7333\n",
      "Epoch 70/200\n",
      "1500/1500 [==============================] - 83s - loss: 0.0276 - acc: 0.9942 - val_loss: 0.3100 - val_acc: 0.8667\n",
      "Epoch 71/200\n",
      "1500/1500 [==============================] - 81s - loss: 0.0216 - acc: 0.9938 - val_loss: 3.9682 - val_acc: 0.6667\n",
      "Epoch 72/200\n",
      "1500/1500 [==============================] - 83s - loss: 0.0227 - acc: 0.9958 - val_loss: 2.1014 - val_acc: 0.8000\n",
      "Epoch 73/200\n",
      "1500/1500 [==============================] - 82s - loss: 0.0240 - acc: 0.9942 - val_loss: 2.5144 - val_acc: 0.7333\n",
      "Epoch 74/200\n",
      "1500/1500 [==============================] - 81s - loss: 0.0249 - acc: 0.9944 - val_loss: 4.0579 - val_acc: 0.6667\n",
      "Epoch 75/200\n",
      "1500/1500 [==============================] - 85s - loss: 0.0194 - acc: 0.9958 - val_loss: 4.2986 - val_acc: 0.7333\n",
      "Epoch 76/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0110 - acc: 0.9980 - val_loss: 2.4271 - val_acc: 0.7333\n",
      "Epoch 77/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0256 - acc: 0.9951 - val_loss: 0.9884 - val_acc: 0.8000\n",
      "Epoch 78/200\n",
      "1500/1500 [==============================] - 85s - loss: 0.0234 - acc: 0.9956 - val_loss: 1.9492 - val_acc: 0.8667\n",
      "Epoch 79/200\n",
      "1500/1500 [==============================] - 85s - loss: 0.0213 - acc: 0.9949 - val_loss: 4.4731 - val_acc: 0.6667\n",
      "Epoch 80/200\n",
      "1500/1500 [==============================] - 77s - loss: 0.0211 - acc: 0.9956 - val_loss: 2.5147 - val_acc: 0.8000\n",
      "Epoch 81/200\n",
      "1500/1500 [==============================] - 86s - loss: 0.0123 - acc: 0.9973 - val_loss: 2.9013 - val_acc: 0.7333\n",
      "Epoch 82/200\n",
      "1500/1500 [==============================] - 86s - loss: 0.0285 - acc: 0.9936 - val_loss: 2.1223 - val_acc: 0.8667\n",
      "Epoch 83/200\n",
      "1500/1500 [==============================] - 85s - loss: 0.0272 - acc: 0.9947 - val_loss: 3.6449 - val_acc: 0.7333\n",
      "Epoch 84/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0277 - acc: 0.9927 - val_loss: 2.5020 - val_acc: 0.7333\n",
      "Epoch 85/200\n",
      "1500/1500 [==============================] - 85s - loss: 0.0327 - acc: 0.9942 - val_loss: 2.8064 - val_acc: 0.8000\n",
      "Epoch 86/200\n",
      "1500/1500 [==============================] - 85s - loss: 0.0294 - acc: 0.9951 - val_loss: 3.2237 - val_acc: 0.8000\n",
      "Epoch 87/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0300 - acc: 0.9944 - val_loss: 3.1076 - val_acc: 0.8000\n",
      "Epoch 88/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0261 - acc: 0.9949 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 89/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0413 - acc: 0.9913 - val_loss: 1.3962 - val_acc: 0.8000\n",
      "Epoch 90/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0091 - acc: 0.9976 - val_loss: 0.5416 - val_acc: 0.8667\n",
      "Epoch 91/200\n",
      "1500/1500 [==============================] - 86s - loss: 0.0298 - acc: 0.9949 - val_loss: 3.7024 - val_acc: 0.7333\n",
      "Epoch 92/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0232 - acc: 0.9953 - val_loss: 3.2664 - val_acc: 0.8000\n",
      "Epoch 93/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0213 - acc: 0.9960 - val_loss: 3.3345 - val_acc: 0.7333\n",
      "Epoch 94/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0433 - acc: 0.9922 - val_loss: 3.5760 - val_acc: 0.6667\n",
      "Epoch 95/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0394 - acc: 0.9940 - val_loss: 2.8150 - val_acc: 0.7333\n",
      "Epoch 96/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0121 - acc: 0.9967 - val_loss: 4.0013 - val_acc: 0.6667\n",
      "Epoch 97/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0400 - acc: 0.9940 - val_loss: 3.7884 - val_acc: 0.6000\n",
      "Epoch 98/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0299 - acc: 0.9944 - val_loss: 3.3003 - val_acc: 0.7333\n",
      "Epoch 99/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0228 - acc: 0.9953 - val_loss: 4.2985 - val_acc: 0.7333\n",
      "Epoch 100/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0315 - acc: 0.9944 - val_loss: 4.3045 - val_acc: 0.7333\n",
      "Epoch 101/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0307 - acc: 0.9942 - val_loss: 4.2986 - val_acc: 0.7333\n",
      "Epoch 102/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0355 - acc: 0.9938 - val_loss: 3.2242 - val_acc: 0.8000\n",
      "Epoch 103/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0321 - acc: 0.9949 - val_loss: 5.3927 - val_acc: 0.6667\n",
      "Epoch 104/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0388 - acc: 0.9936 - val_loss: 2.4923 - val_acc: 0.8000\n",
      "Epoch 105/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0205 - acc: 0.9971 - val_loss: 3.4343 - val_acc: 0.7333\n",
      "Epoch 106/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0333 - acc: 0.9956 - val_loss: 3.4477 - val_acc: 0.7333\n",
      "Epoch 107/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0175 - acc: 0.9969 - val_loss: 3.1039 - val_acc: 0.8000\n",
      "Epoch 108/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0170 - acc: 0.9971 - val_loss: 3.6552 - val_acc: 0.7333\n",
      "Epoch 109/200\n",
      "1500/1500 [==============================] - 87s - loss: 0.0242 - acc: 0.9962 - val_loss: 5.3727 - val_acc: 0.6667\n",
      "Epoch 110/200\n",
      "1500/1500 [==============================] - 84s - loss: 0.0312 - acc: 0.9938 - val_loss: 4.3254 - val_acc: 0.7333\n",
      "Epoch 111/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0342 - acc: 0.9947 - val_loss: 3.2471 - val_acc: 0.8000\n",
      "Epoch 112/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0378 - acc: 0.9942 - val_loss: 3.2236 - val_acc: 0.8000\n",
      "Epoch 113/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0235 - acc: 0.9951 - val_loss: 3.3348 - val_acc: 0.7333\n",
      "Epoch 114/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0472 - acc: 0.9942 - val_loss: 4.5905 - val_acc: 0.6667\n",
      "Epoch 115/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0357 - acc: 0.9953 - val_loss: 2.1704 - val_acc: 0.8667\n",
      "Epoch 116/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0227 - acc: 0.9964 - val_loss: 4.3113 - val_acc: 0.7333\n",
      "Epoch 117/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0273 - acc: 0.9962 - val_loss: 4.2996 - val_acc: 0.7333\n",
      "Epoch 118/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0297 - acc: 0.9951 - val_loss: 1.0745 - val_acc: 0.9333\n",
      "Epoch 119/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0401 - acc: 0.9947 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 120/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0256 - acc: 0.9958 - val_loss: 1.3639 - val_acc: 0.8667\n",
      "Epoch 121/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0115 - acc: 0.9982 - val_loss: 3.2236 - val_acc: 0.8000\n",
      "Epoch 122/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0146 - acc: 0.9976 - val_loss: 2.7378 - val_acc: 0.8000\n",
      "Epoch 123/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0234 - acc: 0.9960 - val_loss: 3.5006 - val_acc: 0.7333\n",
      "Epoch 124/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0273 - acc: 0.9942 - val_loss: 4.0187 - val_acc: 0.7333\n",
      "Epoch 125/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0340 - acc: 0.9940 - val_loss: 3.4557 - val_acc: 0.7333\n",
      "Epoch 126/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0400 - acc: 0.9942 - val_loss: 2.1492 - val_acc: 0.8667\n",
      "Epoch 127/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0186 - acc: 0.9967 - val_loss: 2.2879 - val_acc: 0.8000\n",
      "Epoch 128/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0409 - acc: 0.9938 - val_loss: 2.1504 - val_acc: 0.8667\n",
      "Epoch 129/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0179 - acc: 0.9960 - val_loss: 1.2812 - val_acc: 0.8667\n",
      "Epoch 130/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0273 - acc: 0.9953 - val_loss: 3.5552 - val_acc: 0.7333\n",
      "Epoch 131/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0134 - acc: 0.9978 - val_loss: 3.2481 - val_acc: 0.8000\n",
      "Epoch 132/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0433 - acc: 0.9931 - val_loss: 2.2027 - val_acc: 0.8000\n",
      "Epoch 133/200\n",
      "1500/1500 [==============================] - 90s - loss: 0.0144 - acc: 0.9978 - val_loss: 2.3042 - val_acc: 0.8000\n",
      "Epoch 134/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0247 - acc: 0.9944 - val_loss: 1.0750 - val_acc: 0.9333\n",
      "Epoch 135/200\n",
      "1500/1500 [==============================] - 88s - loss: 0.0309 - acc: 0.9944 - val_loss: 2.1891 - val_acc: 0.8667\n",
      "Epoch 136/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0210 - acc: 0.9956 - val_loss: 2.1735 - val_acc: 0.8667\n",
      "Epoch 137/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0248 - acc: 0.9947 - val_loss: 3.3643 - val_acc: 0.6667\n",
      "Epoch 138/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0241 - acc: 0.9947 - val_loss: 3.3873 - val_acc: 0.7333\n",
      "Epoch 139/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0356 - acc: 0.9949 - val_loss: 3.6065 - val_acc: 0.7333\n",
      "Epoch 140/200\n",
      "1500/1500 [==============================] - 93s - loss: 0.0205 - acc: 0.9969 - val_loss: 3.0324 - val_acc: 0.7333\n",
      "Epoch 141/200\n",
      "1500/1500 [==============================] - 93s - loss: 0.0381 - acc: 0.9956 - val_loss: 3.2265 - val_acc: 0.8000\n",
      "Epoch 142/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0191 - acc: 0.9973 - val_loss: 2.1631 - val_acc: 0.8667\n",
      "Epoch 143/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0098 - acc: 0.9978 - val_loss: 2.5572 - val_acc: 0.8000\n",
      "Epoch 144/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0228 - acc: 0.9967 - val_loss: 2.6162 - val_acc: 0.7333\n",
      "Epoch 145/200\n",
      "1500/1500 [==============================] - 93s - loss: 0.0445 - acc: 0.9940 - val_loss: 1.6487 - val_acc: 0.7333\n",
      "Epoch 146/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0351 - acc: 0.9944 - val_loss: 1.1503 - val_acc: 0.9333\n",
      "Epoch 147/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0364 - acc: 0.9953 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 148/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0230 - acc: 0.9964 - val_loss: 4.7881 - val_acc: 0.6667\n",
      "Epoch 149/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0251 - acc: 0.9951 - val_loss: 2.1970 - val_acc: 0.8667\n",
      "Epoch 150/200\n",
      "1500/1500 [==============================] - 92s - loss: 0.0067 - acc: 0.9982 - val_loss: 3.2258 - val_acc: 0.8000\n",
      "Epoch 151/200\n",
      "1500/1500 [==============================] - 93s - loss: 0.0158 - acc: 0.9978 - val_loss: 7.5350 - val_acc: 0.5333\n",
      "Epoch 152/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0238 - acc: 0.9958 - val_loss: 3.2016 - val_acc: 0.8000\n",
      "Epoch 153/200\n",
      "1500/1500 [==============================] - 89s - loss: 0.0232 - acc: 0.9944 - val_loss: 2.4579 - val_acc: 0.7333\n",
      "Epoch 154/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0313 - acc: 0.9947 - val_loss: 3.8892 - val_acc: 0.7333\n",
      "Epoch 155/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0132 - acc: 0.9971 - val_loss: 2.9866 - val_acc: 0.8000\n",
      "Epoch 156/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0425 - acc: 0.9951 - val_loss: 3.9411 - val_acc: 0.7333\n",
      "Epoch 157/200\n",
      "1500/1500 [==============================] - 91s - loss: 0.0249 - acc: 0.9964 - val_loss: 4.2556 - val_acc: 0.7333\n",
      "Epoch 158/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0268 - acc: 0.9969 - val_loss: 3.2270 - val_acc: 0.8000\n",
      "Epoch 159/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0567 - acc: 0.9936 - val_loss: 3.6129 - val_acc: 0.7333\n",
      "Epoch 160/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0066 - acc: 0.9987 - val_loss: 3.3754 - val_acc: 0.7333\n",
      "Epoch 161/200\n",
      "1500/1500 [==============================] - 98s - loss: 0.0379 - acc: 0.9949 - val_loss: 3.3340 - val_acc: 0.7333\n",
      "Epoch 162/200\n",
      "1500/1500 [==============================] - 103s - loss: 0.0298 - acc: 0.9958 - val_loss: 4.3152 - val_acc: 0.6667\n",
      "Epoch 163/200\n",
      "1500/1500 [==============================] - 99s - loss: 0.0437 - acc: 0.9953 - val_loss: 4.2982 - val_acc: 0.7333\n",
      "Epoch 164/200\n",
      "1500/1500 [==============================] - 97s - loss: 0.0311 - acc: 0.9964 - val_loss: 4.2734 - val_acc: 0.7333\n",
      "Epoch 165/200\n",
      "1500/1500 [==============================] - 96s - loss: 0.0380 - acc: 0.9962 - val_loss: 4.1065 - val_acc: 0.7333\n",
      "Epoch 166/200\n",
      "1500/1500 [==============================] - 94s - loss: 0.0641 - acc: 0.9938 - val_loss: 3.2266 - val_acc: 0.8000\n",
      "Epoch 167/200\n",
      "1500/1500 [==============================] - 96s - loss: 0.0372 - acc: 0.9958 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 168/200\n",
      "1500/1500 [==============================] - 96s - loss: 0.0384 - acc: 0.9953 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 169/200\n",
      "1500/1500 [==============================] - 95s - loss: 0.0224 - acc: 0.9973 - val_loss: 3.7714 - val_acc: 0.7333\n",
      "Epoch 170/200\n",
      "1500/1500 [==============================] - 97s - loss: 0.0242 - acc: 0.9971 - val_loss: 4.2722 - val_acc: 0.7333\n",
      "Epoch 171/200\n",
      "1500/1500 [==============================] - 97s - loss: 0.0404 - acc: 0.9958 - val_loss: 3.4010 - val_acc: 0.7333\n",
      "Epoch 172/200\n",
      "1500/1500 [==============================] - 97s - loss: 0.0276 - acc: 0.9967 - val_loss: 5.3727 - val_acc: 0.6667\n",
      "Epoch 173/200\n",
      "1500/1500 [==============================] - 97s - loss: 0.0227 - acc: 0.9967 - val_loss: 3.2236 - val_acc: 0.8000\n",
      "Epoch 174/200\n",
      "1500/1500 [==============================] - 98s - loss: 0.0302 - acc: 0.9962 - val_loss: 4.2982 - val_acc: 0.7333\n",
      "Epoch 175/200\n",
      "1500/1500 [==============================] - 97s - loss: 0.0400 - acc: 0.9947 - val_loss: 2.2688 - val_acc: 0.8000\n",
      "Epoch 176/200\n",
      "1500/1500 [==============================] - 99s - loss: 0.0149 - acc: 0.9987 - val_loss: 2.1968 - val_acc: 0.8000\n",
      "Epoch 177/200\n",
      "1500/1500 [==============================] - 98s - loss: 0.0259 - acc: 0.9976 - val_loss: 2.5157 - val_acc: 0.8000\n",
      "Epoch 178/200\n",
      "1500/1500 [==============================] - 96s - loss: 0.0315 - acc: 0.9942 - val_loss: 2.8973 - val_acc: 0.7333\n",
      "Epoch 179/200\n",
      "1500/1500 [==============================] - 98s - loss: 0.0522 - acc: 0.9947 - val_loss: 3.5966 - val_acc: 0.6667\n",
      "Epoch 180/200\n",
      "1500/1500 [==============================] - 99s - loss: 0.0697 - acc: 0.9929 - val_loss: 3.2236 - val_acc: 0.8000\n",
      "Epoch 181/200\n",
      "1500/1500 [==============================] - 101s - loss: 0.0108 - acc: 0.9984 - val_loss: 2.6834 - val_acc: 0.7333\n",
      "Epoch 182/200\n",
      "1500/1500 [==============================] - 99s - loss: 0.0395 - acc: 0.9951 - val_loss: 3.2732 - val_acc: 0.8000\n",
      "Epoch 183/200\n",
      "1500/1500 [==============================] - 103s - loss: 0.0167 - acc: 0.9980 - val_loss: 3.1645 - val_acc: 0.7333\n",
      "Epoch 184/200\n",
      "1500/1500 [==============================] - 104s - loss: 0.0348 - acc: 0.9964 - val_loss: 4.2982 - val_acc: 0.7333\n",
      "Epoch 185/200\n",
      "1500/1500 [==============================] - 103s - loss: 0.0394 - acc: 0.9960 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 186/200\n",
      "1500/1500 [==============================] - 103s - loss: 0.0467 - acc: 0.9962 - val_loss: 3.6262 - val_acc: 0.7333\n",
      "Epoch 187/200\n",
      "1500/1500 [==============================] - 104s - loss: 0.0356 - acc: 0.9960 - val_loss: 3.7846 - val_acc: 0.7333\n",
      "Epoch 188/200\n",
      "1500/1500 [==============================] - 104s - loss: 0.0359 - acc: 0.9958 - val_loss: 2.1491 - val_acc: 0.8667\n",
      "Epoch 189/200\n",
      "1500/1500 [==============================] - 103s - loss: 0.0636 - acc: 0.9936 - val_loss: 3.5410 - val_acc: 0.7333\n",
      "Epoch 190/200\n",
      "1500/1500 [==============================] - 101s - loss: 0.0145 - acc: 0.9978 - val_loss: 3.1202 - val_acc: 0.7333\n",
      "Epoch 191/200\n",
      "1500/1500 [==============================] - 100s - loss: 0.0483 - acc: 0.9956 - val_loss: 3.2239 - val_acc: 0.8000\n",
      "Epoch 192/200\n",
      "1500/1500 [==============================] - 100s - loss: 0.0360 - acc: 0.9953 - val_loss: 3.2236 - val_acc: 0.8000\n",
      "Epoch 193/200\n",
      "1500/1500 [==============================] - 102s - loss: 0.0097 - acc: 0.9991 - val_loss: 3.4774 - val_acc: 0.7333\n",
      "Epoch 194/200\n",
      "1500/1500 [==============================] - 101s - loss: 0.0794 - acc: 0.9931 - val_loss: 4.2982 - val_acc: 0.7333\n",
      "Epoch 195/200\n",
      "1500/1500 [==============================] - 100s - loss: 0.0265 - acc: 0.9973 - val_loss: 3.2236 - val_acc: 0.8000\n",
      "Epoch 196/200\n",
      "1500/1500 [==============================] - 99s - loss: 0.0698 - acc: 0.9942 - val_loss: 4.7934 - val_acc: 0.6667\n",
      "Epoch 197/200\n",
      "1500/1500 [==============================] - 100s - loss: 0.0517 - acc: 0.9951 - val_loss: 3.3569 - val_acc: 0.6667\n",
      "Epoch 198/200\n",
      "1500/1500 [==============================] - 101s - loss: 0.0371 - acc: 0.9958 - val_loss: 3.4091 - val_acc: 0.6667\n",
      "Epoch 199/200\n",
      "1500/1500 [==============================] - 101s - loss: 0.0611 - acc: 0.9949 - val_loss: 3.0024 - val_acc: 0.8000\n",
      "Epoch 200/200\n",
      "1500/1500 [==============================] - 101s - loss: 0.0534 - acc: 0.9944 - val_loss: 4.1420 - val_acc: 0.7333\n",
      "-- Evaluate --\n",
      "acc: 73.33%\n",
      "-- Predict --\n",
      "[[1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [1.000 0.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(3)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=10,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.7,\n",
    "                                   zoom_range=[0.9, 2.2],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/train',\n",
    "        target_size=(24, 24),\n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/test',\n",
    "        target_size=(24, 24),    \n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(24,24,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습시키기\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=15 * 100,\n",
    "        epochs=200,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=5)\n",
    "\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            test_generator, \n",
    "            steps = 5)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "\n",
    "output = model.predict_generator(\n",
    "            test_generator, \n",
    "            steps = 5)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73.33%의 정확도를 얻었습니다. 만족할만한 수준은 아니지만, 도전 시험셋으로 기존 모델을 시험했을 때의 결과가 50%를 못 미치는 수준에 비하면 비약적인 개선이 일어났습니다. 이는 동일한 모델을 사용하면서 훈련 데이터만 부풀려서 학습을 시켰을 뿐인데 성능 향상이 일어났습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 결론\n",
    "\n",
    "원, 삼각형, 사각형을 분류하는 간단한 문제에서도 개발 모델이 현실에 적용하기 위해서는 어떠한 어려움이 있는 지 알게되었습니다. 그리고 이를 극복하는 방안으로 데이터 부풀리기 방법에 대해서 알아보고, 각 파라미터 별로 어떻게 데이터를 부풀리는 지 생성된 이미지를 통해 살펴보왔습니다. 훈련셋이 충분하지 않거나 시험셋의 다양한 특성을 반영되어 있지 않다면 데이터 부풀리기 방법은 성능 개선에 큰 도움을 줄 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 같이 보기\n",
    "\n",
    "* [강좌 목차](https://tykimos.github.io/Keras/lecture/)\n",
    "* 이전 : [딥러닝 모델 이야기/컨볼루션 신경망 레이어 이야기](https://tykimos.github.io/Keras/2017/01/27/CNN_Layer_Talk/)\n",
    "* 다음 : [딥러닝 모델 이야기/순환 신경망 레이어 이야기]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
