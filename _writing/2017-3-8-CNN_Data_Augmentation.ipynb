{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"컨볼루션 신경망 모델을 위한 데이터 부풀리기\"\n",
    "author: Taeyoung, Kim\n",
    "date:   2017-03-08 23:10:00\n",
    "categories: Lecture\n",
    "comments: true\n",
    "image: http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_2.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 강좌에서는 컨볼루션 신경망 모델의 성능을 높이기 위해서 데이터를 부풀리는 방법에 대해서 알아보겠습니다.\n",
    "\n",
    "1. 데이터 부풀리기\n",
    "1. 문제 정의하기\n",
    "1. 데이터셋 준비하기\n",
    "1. 모델 구성하기\n",
    "1. 모델 엮기\n",
    "1. 모델 학습시키기\n",
    "1. 모델 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 데이터 부풀리기\n",
    "\n",
    "데이터 부풀리는 방법 설명\n",
    "\n",
    "- rotation_range : \n",
    "- width_shift_range : \n",
    "- height_shift_range : \n",
    "- shear_range : \n",
    "- zoom_range : \n",
    "- horizontal_flip : \n",
    "- vertical_flip : \n",
    "- fill_mode : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(3)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/train',\n",
    "        target_size=(24, 24),\n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/test',\n",
    "        target_size=(24, 24),    \n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(12, (5, 5), padding='same', activation='relu', input_shape=(24, 24, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(8, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, (2, 2), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습시키기\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=60,\n",
    "        epochs=200,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=15)\n",
    "\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            test_generator, \n",
    "            steps = 15)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "\n",
    "output = model.predict_generator(\n",
    "            test_generator, \n",
    "            steps = 15)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(5)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=40,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                fill_mode='nearest')\n",
    "\n",
    "img = load_img('/Users/tykimos/Projects/Keras/_writing/warehouse/handwriting_shape/validation/triangle/triangle018.png')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in train_datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='tri', save_format='png'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n",
      "Epoch 1/200\n",
      "60/60 [==============================] - 1s - loss: 1.2168 - acc: 0.3222 - val_loss: 1.0996 - val_acc: 0.3333\n",
      "Epoch 2/200\n",
      "60/60 [==============================] - 0s - loss: 1.0944 - acc: 0.3444 - val_loss: 1.0943 - val_acc: 0.3333\n",
      "Epoch 3/200\n",
      "60/60 [==============================] - 0s - loss: 1.0561 - acc: 0.4278 - val_loss: 1.1075 - val_acc: 0.3778\n",
      "Epoch 4/200\n",
      "60/60 [==============================] - 0s - loss: 0.9987 - acc: 0.4889 - val_loss: 1.0203 - val_acc: 0.4222\n",
      "Epoch 5/200\n",
      "60/60 [==============================] - 0s - loss: 0.9639 - acc: 0.5222 - val_loss: 1.0399 - val_acc: 0.4444\n",
      "Epoch 6/200\n",
      "60/60 [==============================] - 0s - loss: 0.8598 - acc: 0.6167 - val_loss: 1.0374 - val_acc: 0.6000\n",
      "Epoch 7/200\n",
      "60/60 [==============================] - 0s - loss: 0.8315 - acc: 0.6167 - val_loss: 1.0036 - val_acc: 0.4667\n",
      "Epoch 8/200\n",
      "60/60 [==============================] - 0s - loss: 0.6910 - acc: 0.7111 - val_loss: 1.2192 - val_acc: 0.6667\n",
      "Epoch 9/200\n",
      "60/60 [==============================] - 0s - loss: 0.5928 - acc: 0.7444 - val_loss: 1.4293 - val_acc: 0.6889\n",
      "Epoch 10/200\n",
      "60/60 [==============================] - 0s - loss: 0.5091 - acc: 0.8000 - val_loss: 1.2396 - val_acc: 0.6000\n",
      "Epoch 11/200\n",
      "60/60 [==============================] - 0s - loss: 0.6079 - acc: 0.7556 - val_loss: 1.2392 - val_acc: 0.6000\n",
      "Epoch 12/200\n",
      "60/60 [==============================] - 1s - loss: 0.4528 - acc: 0.8278 - val_loss: 1.3537 - val_acc: 0.5333\n",
      "Epoch 13/200\n",
      "60/60 [==============================] - 1s - loss: 0.4920 - acc: 0.8000 - val_loss: 1.8258 - val_acc: 0.4667\n",
      "Epoch 14/200\n",
      "60/60 [==============================] - 1s - loss: 0.4159 - acc: 0.8056 - val_loss: 1.7957 - val_acc: 0.5333\n",
      "Epoch 15/200\n",
      "60/60 [==============================] - 1s - loss: 0.4640 - acc: 0.8333 - val_loss: 1.6894 - val_acc: 0.4000\n",
      "Epoch 16/200\n",
      "60/60 [==============================] - 1s - loss: 0.3689 - acc: 0.8778 - val_loss: 2.2206 - val_acc: 0.3778\n",
      "Epoch 17/200\n",
      "60/60 [==============================] - 1s - loss: 0.4490 - acc: 0.8389 - val_loss: 1.6966 - val_acc: 0.4667\n",
      "Epoch 18/200\n",
      "60/60 [==============================] - 1s - loss: 0.3591 - acc: 0.8778 - val_loss: 2.3316 - val_acc: 0.4000\n",
      "Epoch 19/200\n",
      "60/60 [==============================] - 1s - loss: 0.3119 - acc: 0.8889 - val_loss: 2.0428 - val_acc: 0.4444\n",
      "Epoch 20/200\n",
      "60/60 [==============================] - 1s - loss: 0.3529 - acc: 0.8889 - val_loss: 2.5535 - val_acc: 0.3333\n",
      "Epoch 21/200\n",
      "60/60 [==============================] - 1s - loss: 0.3738 - acc: 0.8667 - val_loss: 1.8533 - val_acc: 0.5556\n",
      "Epoch 22/200\n",
      "60/60 [==============================] - 1s - loss: 0.3777 - acc: 0.8944 - val_loss: 1.8613 - val_acc: 0.4667\n",
      "Epoch 23/200\n",
      "60/60 [==============================] - 1s - loss: 0.2611 - acc: 0.9167 - val_loss: 2.5668 - val_acc: 0.4222\n",
      "Epoch 24/200\n",
      "60/60 [==============================] - 1s - loss: 0.2254 - acc: 0.9278 - val_loss: 1.9959 - val_acc: 0.4667\n",
      "Epoch 25/200\n",
      "60/60 [==============================] - 1s - loss: 0.2748 - acc: 0.8722 - val_loss: 2.1304 - val_acc: 0.4000\n",
      "Epoch 26/200\n",
      "60/60 [==============================] - 1s - loss: 0.2316 - acc: 0.8889 - val_loss: 1.9599 - val_acc: 0.4667\n",
      "Epoch 27/200\n",
      "60/60 [==============================] - 2s - loss: 0.1867 - acc: 0.9278 - val_loss: 2.9171 - val_acc: 0.5333\n",
      "Epoch 28/200\n",
      "60/60 [==============================] - 1s - loss: 0.2530 - acc: 0.9278 - val_loss: 1.8509 - val_acc: 0.4667\n",
      "Epoch 29/200\n",
      "60/60 [==============================] - 1s - loss: 0.3223 - acc: 0.9056 - val_loss: 2.2283 - val_acc: 0.4667\n",
      "Epoch 30/200\n",
      "60/60 [==============================] - 1s - loss: 0.2620 - acc: 0.9167 - val_loss: 1.7626 - val_acc: 0.4889\n",
      "Epoch 31/200\n",
      "60/60 [==============================] - 1s - loss: 0.2338 - acc: 0.9167 - val_loss: 2.3415 - val_acc: 0.5333\n",
      "Epoch 32/200\n",
      "60/60 [==============================] - 1s - loss: 0.2190 - acc: 0.9167 - val_loss: 1.7238 - val_acc: 0.7111\n",
      "Epoch 33/200\n",
      "60/60 [==============================] - 1s - loss: 0.2312 - acc: 0.9111 - val_loss: 1.9603 - val_acc: 0.5333\n",
      "Epoch 34/200\n",
      "60/60 [==============================] - 1s - loss: 0.2042 - acc: 0.9111 - val_loss: 1.7895 - val_acc: 0.4667\n",
      "Epoch 35/200\n",
      "60/60 [==============================] - 1s - loss: 0.0829 - acc: 0.9722 - val_loss: 2.1152 - val_acc: 0.4000\n",
      "Epoch 36/200\n",
      "60/60 [==============================] - 1s - loss: 0.2329 - acc: 0.9222 - val_loss: 2.9672 - val_acc: 0.5111\n",
      "Epoch 37/200\n",
      "60/60 [==============================] - 1s - loss: 0.2154 - acc: 0.9000 - val_loss: 1.5765 - val_acc: 0.3556\n",
      "Epoch 38/200\n",
      "60/60 [==============================] - 2s - loss: 0.2740 - acc: 0.9167 - val_loss: 2.3088 - val_acc: 0.4444\n",
      "Epoch 39/200\n",
      "60/60 [==============================] - 1s - loss: 0.2963 - acc: 0.8833 - val_loss: 2.2453 - val_acc: 0.5333\n",
      "Epoch 40/200\n",
      "60/60 [==============================] - 1s - loss: 0.1957 - acc: 0.9222 - val_loss: 2.3110 - val_acc: 0.6667\n",
      "Epoch 41/200\n",
      "60/60 [==============================] - 1s - loss: 0.1986 - acc: 0.9556 - val_loss: 2.0469 - val_acc: 0.5111\n",
      "Epoch 42/200\n",
      "60/60 [==============================] - 1s - loss: 0.1992 - acc: 0.9000 - val_loss: 2.1349 - val_acc: 0.5333\n",
      "Epoch 43/200\n",
      "60/60 [==============================] - 1s - loss: 0.2127 - acc: 0.9667 - val_loss: 2.2674 - val_acc: 0.4889\n",
      "Epoch 44/200\n",
      "60/60 [==============================] - 1s - loss: 0.1552 - acc: 0.9333 - val_loss: 2.6653 - val_acc: 0.5333\n",
      "Epoch 45/200\n",
      "60/60 [==============================] - 1s - loss: 0.1750 - acc: 0.9556 - val_loss: 2.1658 - val_acc: 0.6000\n",
      "Epoch 46/200\n",
      "60/60 [==============================] - 1s - loss: 0.1850 - acc: 0.9333 - val_loss: 1.9594 - val_acc: 0.6222\n",
      "Epoch 47/200\n",
      "60/60 [==============================] - 1s - loss: 0.2522 - acc: 0.9167 - val_loss: 2.4895 - val_acc: 0.5778\n",
      "Epoch 48/200\n",
      "60/60 [==============================] - 1s - loss: 0.1618 - acc: 0.9500 - val_loss: 2.1903 - val_acc: 0.5111\n",
      "Epoch 49/200\n",
      "60/60 [==============================] - 1s - loss: 0.1941 - acc: 0.9333 - val_loss: 2.3925 - val_acc: 0.5111\n",
      "Epoch 50/200\n",
      "60/60 [==============================] - 1s - loss: 0.2715 - acc: 0.8944 - val_loss: 2.4266 - val_acc: 0.5333\n",
      "Epoch 51/200\n",
      "60/60 [==============================] - 1s - loss: 0.2188 - acc: 0.9167 - val_loss: 2.4954 - val_acc: 0.4222\n",
      "Epoch 52/200\n",
      "60/60 [==============================] - 2s - loss: 0.1725 - acc: 0.9278 - val_loss: 2.5294 - val_acc: 0.5333\n",
      "Epoch 53/200\n",
      "60/60 [==============================] - 2s - loss: 0.2126 - acc: 0.9333 - val_loss: 2.6749 - val_acc: 0.4000\n",
      "Epoch 54/200\n",
      "60/60 [==============================] - 2s - loss: 0.2030 - acc: 0.9167 - val_loss: 2.1543 - val_acc: 0.6000\n",
      "Epoch 55/200\n",
      "60/60 [==============================] - 2s - loss: 0.1605 - acc: 0.9333 - val_loss: 2.2336 - val_acc: 0.5556\n",
      "Epoch 56/200\n",
      "60/60 [==============================] - 2s - loss: 0.2137 - acc: 0.9111 - val_loss: 2.3518 - val_acc: 0.4667\n",
      "Epoch 57/200\n",
      "60/60 [==============================] - 2s - loss: 0.1309 - acc: 0.9444 - val_loss: 2.3337 - val_acc: 0.5333\n",
      "Epoch 58/200\n",
      "60/60 [==============================] - 2s - loss: 0.1768 - acc: 0.9667 - val_loss: 2.8581 - val_acc: 0.6000\n",
      "Epoch 59/200\n",
      "60/60 [==============================] - 2s - loss: 0.1467 - acc: 0.9500 - val_loss: 2.7607 - val_acc: 0.5778\n",
      "Epoch 60/200\n",
      "60/60 [==============================] - 2s - loss: 0.1417 - acc: 0.9500 - val_loss: 2.2573 - val_acc: 0.5333\n",
      "Epoch 61/200\n",
      "60/60 [==============================] - 1s - loss: 0.1164 - acc: 0.9444 - val_loss: 2.4408 - val_acc: 0.4222\n",
      "Epoch 62/200\n",
      "60/60 [==============================] - 2s - loss: 0.1922 - acc: 0.9111 - val_loss: 2.3858 - val_acc: 0.6000\n",
      "Epoch 63/200\n",
      "60/60 [==============================] - 1s - loss: 0.1777 - acc: 0.9444 - val_loss: 1.5636 - val_acc: 0.6667\n",
      "Epoch 64/200\n",
      "60/60 [==============================] - 1s - loss: 0.2347 - acc: 0.9278 - val_loss: 2.2016 - val_acc: 0.7111\n",
      "Epoch 65/200\n",
      "60/60 [==============================] - 1s - loss: 0.1019 - acc: 0.9722 - val_loss: 2.2801 - val_acc: 0.5778\n",
      "Epoch 66/200\n",
      "60/60 [==============================] - 1s - loss: 0.1111 - acc: 0.9389 - val_loss: 2.3318 - val_acc: 0.6667\n",
      "Epoch 67/200\n",
      "60/60 [==============================] - 1s - loss: 0.1141 - acc: 0.9722 - val_loss: 2.1098 - val_acc: 0.6889\n",
      "Epoch 68/200\n",
      "60/60 [==============================] - 1s - loss: 0.1138 - acc: 0.9667 - val_loss: 1.5310 - val_acc: 0.6667\n",
      "Epoch 69/200\n",
      "60/60 [==============================] - 1s - loss: 0.2220 - acc: 0.9389 - val_loss: 2.3916 - val_acc: 0.6889\n",
      "Epoch 70/200\n",
      "60/60 [==============================] - 1s - loss: 0.1591 - acc: 0.9278 - val_loss: 2.1872 - val_acc: 0.4667\n",
      "Epoch 71/200\n",
      "60/60 [==============================] - 1s - loss: 0.1440 - acc: 0.9500 - val_loss: 2.5016 - val_acc: 0.5333\n",
      "Epoch 72/200\n",
      "60/60 [==============================] - 2s - loss: 0.0909 - acc: 0.9667 - val_loss: 3.0547 - val_acc: 0.6000\n",
      "Epoch 73/200\n",
      "60/60 [==============================] - 2s - loss: 0.2208 - acc: 0.9222 - val_loss: 2.6332 - val_acc: 0.4667\n",
      "Epoch 74/200\n",
      "60/60 [==============================] - 2s - loss: 0.1757 - acc: 0.9333 - val_loss: 2.4554 - val_acc: 0.6222\n",
      "Epoch 75/200\n",
      "60/60 [==============================] - 2s - loss: 0.0969 - acc: 0.9722 - val_loss: 2.4323 - val_acc: 0.7333\n",
      "Epoch 76/200\n",
      "60/60 [==============================] - 2s - loss: 0.2048 - acc: 0.9444 - val_loss: 2.5427 - val_acc: 0.5333\n",
      "Epoch 77/200\n",
      "60/60 [==============================] - 2s - loss: 0.1557 - acc: 0.9444 - val_loss: 2.6301 - val_acc: 0.5333\n",
      "Epoch 78/200\n",
      "60/60 [==============================] - 2s - loss: 0.2045 - acc: 0.9222 - val_loss: 2.5565 - val_acc: 0.5556\n",
      "Epoch 79/200\n",
      "60/60 [==============================] - 2s - loss: 0.1894 - acc: 0.9500 - val_loss: 1.9728 - val_acc: 0.5333\n",
      "Epoch 80/200\n",
      "60/60 [==============================] - 2s - loss: 0.1408 - acc: 0.9389 - val_loss: 2.7187 - val_acc: 0.6667\n",
      "Epoch 81/200\n",
      "60/60 [==============================] - 2s - loss: 0.1765 - acc: 0.9500 - val_loss: 3.0032 - val_acc: 0.5778\n",
      "Epoch 82/200\n",
      "60/60 [==============================] - 2s - loss: 0.1145 - acc: 0.9611 - val_loss: 2.1541 - val_acc: 0.5556\n",
      "Epoch 83/200\n",
      "60/60 [==============================] - 1s - loss: 0.0543 - acc: 0.9722 - val_loss: 2.7890 - val_acc: 0.5111\n",
      "Epoch 84/200\n",
      "60/60 [==============================] - 2s - loss: 0.1007 - acc: 0.9722 - val_loss: 2.7664 - val_acc: 0.5333\n",
      "Epoch 85/200\n",
      "60/60 [==============================] - 2s - loss: 0.1254 - acc: 0.9389 - val_loss: 3.1620 - val_acc: 0.6667\n",
      "Epoch 86/200\n",
      "60/60 [==============================] - 1s - loss: 0.1564 - acc: 0.9500 - val_loss: 2.3989 - val_acc: 0.6667\n",
      "Epoch 87/200\n",
      "60/60 [==============================] - 2s - loss: 0.1420 - acc: 0.9500 - val_loss: 2.6005 - val_acc: 0.5333\n",
      "Epoch 88/200\n",
      "60/60 [==============================] - 2s - loss: 0.1085 - acc: 0.9556 - val_loss: 3.1981 - val_acc: 0.6000\n",
      "Epoch 89/200\n",
      "60/60 [==============================] - 2s - loss: 0.2456 - acc: 0.9333 - val_loss: 3.4627 - val_acc: 0.4444\n",
      "Epoch 90/200\n",
      "60/60 [==============================] - 2s - loss: 0.1571 - acc: 0.9389 - val_loss: 2.2904 - val_acc: 0.5111\n",
      "Epoch 91/200\n",
      "60/60 [==============================] - 2s - loss: 0.1750 - acc: 0.9333 - val_loss: 2.1372 - val_acc: 0.4222\n",
      "Epoch 92/200\n",
      "60/60 [==============================] - 2s - loss: 0.2406 - acc: 0.9222 - val_loss: 2.2477 - val_acc: 0.5111\n",
      "Epoch 93/200\n",
      "60/60 [==============================] - 2s - loss: 0.1471 - acc: 0.9500 - val_loss: 1.9549 - val_acc: 0.4667\n",
      "Epoch 94/200\n",
      "60/60 [==============================] - 2s - loss: 0.0962 - acc: 0.9556 - val_loss: 2.3362 - val_acc: 0.5556\n",
      "Epoch 95/200\n",
      "60/60 [==============================] - 2s - loss: 0.1430 - acc: 0.9611 - val_loss: 2.3197 - val_acc: 0.5556\n",
      "Epoch 96/200\n",
      "60/60 [==============================] - 2s - loss: 0.1309 - acc: 0.9556 - val_loss: 2.6123 - val_acc: 0.4667\n",
      "Epoch 97/200\n",
      "60/60 [==============================] - 2s - loss: 0.1635 - acc: 0.9389 - val_loss: 2.0643 - val_acc: 0.5778\n",
      "Epoch 98/200\n",
      "60/60 [==============================] - 2s - loss: 0.2162 - acc: 0.9333 - val_loss: 2.2882 - val_acc: 0.4000\n",
      "Epoch 99/200\n",
      "60/60 [==============================] - 2s - loss: 0.1657 - acc: 0.9444 - val_loss: 2.5430 - val_acc: 0.6000\n",
      "Epoch 100/200\n",
      "60/60 [==============================] - 2s - loss: 0.1314 - acc: 0.9500 - val_loss: 2.5686 - val_acc: 0.5778\n",
      "Epoch 101/200\n",
      "60/60 [==============================] - 2s - loss: 0.0828 - acc: 0.9722 - val_loss: 1.8803 - val_acc: 0.5556\n",
      "Epoch 102/200\n",
      "60/60 [==============================] - 2s - loss: 0.1420 - acc: 0.9444 - val_loss: 1.8513 - val_acc: 0.5333\n",
      "Epoch 103/200\n",
      "60/60 [==============================] - 2s - loss: 0.0936 - acc: 0.9722 - val_loss: 2.1191 - val_acc: 0.5111\n",
      "Epoch 104/200\n",
      "60/60 [==============================] - 2s - loss: 0.1074 - acc: 0.9778 - val_loss: 2.9246 - val_acc: 0.6000\n",
      "Epoch 105/200\n",
      "60/60 [==============================] - 2s - loss: 0.2010 - acc: 0.9500 - val_loss: 2.8208 - val_acc: 0.6000\n",
      "Epoch 106/200\n",
      "60/60 [==============================] - 2s - loss: 0.1280 - acc: 0.9667 - val_loss: 2.4361 - val_acc: 0.5333\n",
      "Epoch 107/200\n",
      "60/60 [==============================] - 1s - loss: 0.1380 - acc: 0.9444 - val_loss: 2.1429 - val_acc: 0.5333\n",
      "Epoch 108/200\n",
      "60/60 [==============================] - 2s - loss: 0.0537 - acc: 0.9889 - val_loss: 1.9425 - val_acc: 0.7778\n",
      "Epoch 109/200\n",
      "60/60 [==============================] - 2s - loss: 0.0440 - acc: 0.9889 - val_loss: 2.2543 - val_acc: 0.6667\n",
      "Epoch 110/200\n",
      "60/60 [==============================] - 1s - loss: 0.1764 - acc: 0.9222 - val_loss: 2.8934 - val_acc: 0.6667\n",
      "Epoch 111/200\n",
      "60/60 [==============================] - 1s - loss: 0.1413 - acc: 0.9500 - val_loss: 3.1903 - val_acc: 0.6222\n",
      "Epoch 112/200\n",
      "60/60 [==============================] - 2s - loss: 0.1127 - acc: 0.9722 - val_loss: 3.5344 - val_acc: 0.5556\n",
      "Epoch 113/200\n",
      "60/60 [==============================] - 2s - loss: 0.1597 - acc: 0.9278 - val_loss: 2.4628 - val_acc: 0.5333\n",
      "Epoch 114/200\n",
      "60/60 [==============================] - 2s - loss: 0.1337 - acc: 0.9722 - val_loss: 2.5304 - val_acc: 0.6667\n",
      "Epoch 115/200\n",
      "60/60 [==============================] - 2s - loss: 0.0787 - acc: 0.9611 - val_loss: 2.3098 - val_acc: 0.6667\n",
      "Epoch 116/200\n",
      "60/60 [==============================] - 2s - loss: 0.1082 - acc: 0.9667 - val_loss: 2.1681 - val_acc: 0.5778\n",
      "Epoch 117/200\n",
      "60/60 [==============================] - 2s - loss: 0.0896 - acc: 0.9722 - val_loss: 2.9116 - val_acc: 0.4667\n",
      "Epoch 118/200\n",
      "60/60 [==============================] - 2s - loss: 0.0497 - acc: 0.9944 - val_loss: 2.5933 - val_acc: 0.6222\n",
      "Epoch 119/200\n",
      "60/60 [==============================] - 2s - loss: 0.0935 - acc: 0.9556 - val_loss: 3.9041 - val_acc: 0.4444\n",
      "Epoch 120/200\n",
      "60/60 [==============================] - 2s - loss: 0.1189 - acc: 0.9778 - val_loss: 1.9237 - val_acc: 0.5556\n",
      "Epoch 121/200\n",
      "60/60 [==============================] - 1s - loss: 0.0800 - acc: 0.9722 - val_loss: 2.4032 - val_acc: 0.5556\n",
      "Epoch 122/200\n",
      "60/60 [==============================] - 1s - loss: 0.0816 - acc: 0.9667 - val_loss: 2.3271 - val_acc: 0.6222\n",
      "Epoch 123/200\n",
      "60/60 [==============================] - 1s - loss: 0.1460 - acc: 0.9333 - val_loss: 1.7997 - val_acc: 0.3333\n",
      "Epoch 124/200\n",
      "60/60 [==============================] - 1s - loss: 0.0929 - acc: 0.9722 - val_loss: 2.3630 - val_acc: 0.3778\n",
      "Epoch 125/200\n",
      "60/60 [==============================] - 2s - loss: 0.1357 - acc: 0.9667 - val_loss: 3.4032 - val_acc: 0.3778\n",
      "Epoch 126/200\n",
      "60/60 [==============================] - 2s - loss: 0.1428 - acc: 0.9556 - val_loss: 2.7762 - val_acc: 0.4000\n",
      "Epoch 127/200\n",
      "60/60 [==============================] - 2s - loss: 0.2037 - acc: 0.9167 - val_loss: 2.5835 - val_acc: 0.3778\n",
      "Epoch 128/200\n",
      "60/60 [==============================] - 1s - loss: 0.1025 - acc: 0.9667 - val_loss: 3.0529 - val_acc: 0.4444\n",
      "Epoch 129/200\n",
      "60/60 [==============================] - 2s - loss: 0.1227 - acc: 0.9667 - val_loss: 1.7686 - val_acc: 0.4000\n",
      "Epoch 130/200\n",
      "60/60 [==============================] - 2s - loss: 0.0866 - acc: 0.9667 - val_loss: 1.9173 - val_acc: 0.5333\n",
      "Epoch 131/200\n",
      "60/60 [==============================] - 2s - loss: 0.1144 - acc: 0.9389 - val_loss: 2.5374 - val_acc: 0.5333\n",
      "Epoch 132/200\n",
      "60/60 [==============================] - 2s - loss: 0.1203 - acc: 0.9556 - val_loss: 2.4558 - val_acc: 0.6000\n",
      "Epoch 133/200\n",
      "60/60 [==============================] - 2s - loss: 0.0529 - acc: 0.9778 - val_loss: 3.0630 - val_acc: 0.5333\n",
      "Epoch 134/200\n",
      "60/60 [==============================] - 2s - loss: 0.1377 - acc: 0.9556 - val_loss: 2.1326 - val_acc: 0.4667\n",
      "Epoch 135/200\n",
      "60/60 [==============================] - 2s - loss: 0.1900 - acc: 0.9278 - val_loss: 2.1452 - val_acc: 0.7111\n",
      "Epoch 136/200\n",
      "60/60 [==============================] - 2s - loss: 0.0821 - acc: 0.9722 - val_loss: 2.9551 - val_acc: 0.4889\n",
      "Epoch 137/200\n",
      "60/60 [==============================] - 2s - loss: 0.0454 - acc: 0.9722 - val_loss: 2.5106 - val_acc: 0.5111\n",
      "Epoch 138/200\n",
      "60/60 [==============================] - 2s - loss: 0.0847 - acc: 0.9611 - val_loss: 2.6406 - val_acc: 0.5333\n",
      "Epoch 139/200\n",
      "60/60 [==============================] - 2s - loss: 0.0491 - acc: 0.9778 - val_loss: 2.6556 - val_acc: 0.4889\n",
      "Epoch 140/200\n",
      "60/60 [==============================] - 2s - loss: 0.0880 - acc: 0.9778 - val_loss: 2.5227 - val_acc: 0.5111\n",
      "Epoch 141/200\n",
      "60/60 [==============================] - 2s - loss: 0.1013 - acc: 0.9667 - val_loss: 2.4098 - val_acc: 0.5333\n",
      "Epoch 142/200\n",
      "60/60 [==============================] - 2s - loss: 0.1203 - acc: 0.9556 - val_loss: 1.8885 - val_acc: 0.6889\n",
      "Epoch 143/200\n",
      "60/60 [==============================] - 2s - loss: 0.0900 - acc: 0.9722 - val_loss: 2.2060 - val_acc: 0.7556\n",
      "Epoch 144/200\n",
      "60/60 [==============================] - 2s - loss: 0.1276 - acc: 0.9722 - val_loss: 2.4242 - val_acc: 0.6000\n",
      "Epoch 145/200\n",
      "60/60 [==============================] - 2s - loss: 0.0701 - acc: 0.9722 - val_loss: 2.9947 - val_acc: 0.6000\n",
      "Epoch 146/200\n",
      "60/60 [==============================] - 2s - loss: 0.0529 - acc: 0.9722 - val_loss: 2.0031 - val_acc: 0.5111\n",
      "Epoch 147/200\n",
      "60/60 [==============================] - 2s - loss: 0.0992 - acc: 0.9611 - val_loss: 3.0321 - val_acc: 0.4667\n",
      "Epoch 148/200\n",
      "60/60 [==============================] - 2s - loss: 0.1346 - acc: 0.9500 - val_loss: 2.7219 - val_acc: 0.5778\n",
      "Epoch 149/200\n",
      "60/60 [==============================] - 2s - loss: 0.0786 - acc: 0.9611 - val_loss: 2.9217 - val_acc: 0.5556\n",
      "Epoch 150/200\n",
      "60/60 [==============================] - 2s - loss: 0.0961 - acc: 0.9722 - val_loss: 2.7351 - val_acc: 0.5333\n",
      "Epoch 151/200\n",
      "60/60 [==============================] - 2s - loss: 0.0796 - acc: 0.9667 - val_loss: 3.1191 - val_acc: 0.4667\n",
      "Epoch 152/200\n",
      "60/60 [==============================] - 2s - loss: 0.1224 - acc: 0.9611 - val_loss: 2.1957 - val_acc: 0.4889\n",
      "Epoch 153/200\n",
      "60/60 [==============================] - 2s - loss: 0.0847 - acc: 0.9611 - val_loss: 2.4503 - val_acc: 0.5111\n",
      "Epoch 154/200\n",
      "60/60 [==============================] - 2s - loss: 0.0818 - acc: 0.9778 - val_loss: 2.6740 - val_acc: 0.3778\n",
      "Epoch 155/200\n",
      "60/60 [==============================] - 2s - loss: 0.1636 - acc: 0.9556 - val_loss: 1.9181 - val_acc: 0.6000\n",
      "Epoch 156/200\n",
      "60/60 [==============================] - 2s - loss: 0.0773 - acc: 0.9667 - val_loss: 2.0610 - val_acc: 0.5556\n",
      "Epoch 157/200\n",
      "60/60 [==============================] - 2s - loss: 0.0773 - acc: 0.9667 - val_loss: 2.3973 - val_acc: 0.5111\n",
      "Epoch 158/200\n",
      "60/60 [==============================] - 2s - loss: 0.1226 - acc: 0.9667 - val_loss: 2.9453 - val_acc: 0.6444\n",
      "Epoch 159/200\n",
      "60/60 [==============================] - 2s - loss: 0.2144 - acc: 0.9444 - val_loss: 1.5767 - val_acc: 0.5333\n",
      "Epoch 160/200\n",
      "60/60 [==============================] - 2s - loss: 0.0573 - acc: 0.9778 - val_loss: 1.9486 - val_acc: 0.6222\n",
      "Epoch 161/200\n",
      "60/60 [==============================] - 2s - loss: 0.1098 - acc: 0.9778 - val_loss: 2.2434 - val_acc: 0.5333\n",
      "Epoch 162/200\n",
      "60/60 [==============================] - 2s - loss: 0.0697 - acc: 0.9722 - val_loss: 2.5192 - val_acc: 0.6000\n",
      "Epoch 163/200\n",
      "60/60 [==============================] - 2s - loss: 0.1148 - acc: 0.9722 - val_loss: 2.5678 - val_acc: 0.5333\n",
      "Epoch 164/200\n",
      "60/60 [==============================] - 2s - loss: 0.0608 - acc: 0.9667 - val_loss: 3.0104 - val_acc: 0.5556\n",
      "Epoch 165/200\n",
      "60/60 [==============================] - 2s - loss: 0.0937 - acc: 0.9667 - val_loss: 2.3888 - val_acc: 0.5333\n",
      "Epoch 166/200\n",
      "60/60 [==============================] - 2s - loss: 0.1022 - acc: 0.9611 - val_loss: 3.1390 - val_acc: 0.5333\n",
      "Epoch 167/200\n",
      "60/60 [==============================] - 2s - loss: 0.1229 - acc: 0.9556 - val_loss: 2.5032 - val_acc: 0.5333\n",
      "Epoch 168/200\n",
      "60/60 [==============================] - 2s - loss: 0.0636 - acc: 0.9722 - val_loss: 2.6486 - val_acc: 0.6000\n",
      "Epoch 169/200\n",
      "60/60 [==============================] - 2s - loss: 0.0732 - acc: 0.9611 - val_loss: 2.7584 - val_acc: 0.5556\n",
      "Epoch 170/200\n",
      "60/60 [==============================] - 2s - loss: 0.1036 - acc: 0.9611 - val_loss: 2.8850 - val_acc: 0.4889\n",
      "Epoch 171/200\n",
      "60/60 [==============================] - 2s - loss: 0.1673 - acc: 0.9444 - val_loss: 1.8678 - val_acc: 0.6667\n",
      "Epoch 172/200\n",
      "60/60 [==============================] - 2s - loss: 0.1288 - acc: 0.9611 - val_loss: 3.6835 - val_acc: 0.6000\n",
      "Epoch 173/200\n",
      "60/60 [==============================] - 2s - loss: 0.0596 - acc: 0.9722 - val_loss: 2.6292 - val_acc: 0.6222\n",
      "Epoch 174/200\n",
      "60/60 [==============================] - 2s - loss: 0.0932 - acc: 0.9556 - val_loss: 2.9211 - val_acc: 0.7556\n",
      "Epoch 175/200\n",
      "60/60 [==============================] - 2s - loss: 0.0789 - acc: 0.9778 - val_loss: 2.2723 - val_acc: 0.5333\n",
      "Epoch 176/200\n",
      "60/60 [==============================] - 2s - loss: 0.0677 - acc: 0.9778 - val_loss: 2.3879 - val_acc: 0.5333\n",
      "Epoch 177/200\n",
      "60/60 [==============================] - 2s - loss: 0.0389 - acc: 0.9833 - val_loss: 2.9228 - val_acc: 0.5778\n",
      "Epoch 178/200\n",
      "60/60 [==============================] - 2s - loss: 0.0960 - acc: 0.9722 - val_loss: 3.1348 - val_acc: 0.6000\n",
      "Epoch 179/200\n",
      "60/60 [==============================] - 2s - loss: 0.1389 - acc: 0.9556 - val_loss: 3.7252 - val_acc: 0.4222\n",
      "Epoch 180/200\n",
      "60/60 [==============================] - 2s - loss: 0.1338 - acc: 0.9556 - val_loss: 2.4643 - val_acc: 0.6222\n",
      "Epoch 181/200\n",
      "60/60 [==============================] - 2s - loss: 0.0929 - acc: 0.9722 - val_loss: 2.9702 - val_acc: 0.6667\n",
      "Epoch 182/200\n",
      "60/60 [==============================] - 2s - loss: 0.0885 - acc: 0.9500 - val_loss: 3.0138 - val_acc: 0.5333\n",
      "Epoch 183/200\n",
      "60/60 [==============================] - 2s - loss: 0.1017 - acc: 0.9444 - val_loss: 2.9849 - val_acc: 0.5333\n",
      "Epoch 184/200\n",
      "60/60 [==============================] - 2s - loss: 0.0988 - acc: 0.9778 - val_loss: 3.9005 - val_acc: 0.4444\n",
      "Epoch 185/200\n",
      "60/60 [==============================] - 2s - loss: 0.1308 - acc: 0.9611 - val_loss: 2.6184 - val_acc: 0.6667\n",
      "Epoch 186/200\n",
      "60/60 [==============================] - 2s - loss: 0.0479 - acc: 0.9722 - val_loss: 3.2731 - val_acc: 0.6667\n",
      "Epoch 187/200\n",
      "60/60 [==============================] - 2s - loss: 0.1137 - acc: 0.9500 - val_loss: 2.8010 - val_acc: 0.6889\n",
      "Epoch 188/200\n",
      "60/60 [==============================] - 2s - loss: 0.0798 - acc: 0.9611 - val_loss: 2.8675 - val_acc: 0.6889\n",
      "Epoch 189/200\n",
      "60/60 [==============================] - 2s - loss: 0.0500 - acc: 0.9833 - val_loss: 3.0729 - val_acc: 0.6000\n",
      "Epoch 190/200\n",
      "60/60 [==============================] - 2s - loss: 0.0632 - acc: 0.9778 - val_loss: 3.4054 - val_acc: 0.6000\n",
      "Epoch 191/200\n",
      "60/60 [==============================] - 2s - loss: 0.0459 - acc: 0.9833 - val_loss: 3.1759 - val_acc: 0.6667\n",
      "Epoch 192/200\n",
      "60/60 [==============================] - 2s - loss: 0.1022 - acc: 0.9611 - val_loss: 3.2907 - val_acc: 0.6000\n",
      "Epoch 193/200\n",
      "60/60 [==============================] - 2s - loss: 0.0384 - acc: 0.9889 - val_loss: 3.0111 - val_acc: 0.6222\n",
      "Epoch 194/200\n",
      "60/60 [==============================] - 2s - loss: 0.1402 - acc: 0.9611 - val_loss: 2.5618 - val_acc: 0.6000\n",
      "Epoch 195/200\n",
      "60/60 [==============================] - 2s - loss: 0.1221 - acc: 0.9556 - val_loss: 2.8869 - val_acc: 0.5778\n",
      "Epoch 196/200\n",
      "60/60 [==============================] - 2s - loss: 0.0918 - acc: 0.9667 - val_loss: 2.0452 - val_acc: 0.6222\n",
      "Epoch 197/200\n",
      "60/60 [==============================] - 2s - loss: 0.1014 - acc: 0.9500 - val_loss: 2.7581 - val_acc: 0.6667\n",
      "Epoch 198/200\n",
      "60/60 [==============================] - 2s - loss: 0.1200 - acc: 0.9611 - val_loss: 2.9813 - val_acc: 0.6000\n",
      "Epoch 199/200\n",
      "60/60 [==============================] - 2s - loss: 0.0581 - acc: 0.9833 - val_loss: 2.5201 - val_acc: 0.6000\n",
      "Epoch 200/200\n",
      "60/60 [==============================] - 2s - loss: 0.0952 - acc: 0.9611 - val_loss: 2.8857 - val_acc: 0.6667\n",
      "-- Evaluate --\n",
      "acc: 68.89%\n",
      "-- Predict --\n",
      "[[0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.126 0.113 0.761]\n",
      " [0.126 0.113 0.761]\n",
      " [0.000 0.000 1.000]\n",
      " [0.601 0.000 0.399]\n",
      " [0.005 0.000 0.995]\n",
      " [0.001 0.976 0.023]\n",
      " [0.990 0.000 0.010]\n",
      " [0.003 0.000 0.997]\n",
      " [0.008 0.946 0.047]\n",
      " [0.554 0.000 0.446]\n",
      " [0.193 0.669 0.137]\n",
      " [0.205 0.001 0.794]\n",
      " [0.714 0.000 0.286]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.010 0.001 0.989]\n",
      " [0.554 0.000 0.446]\n",
      " [0.126 0.113 0.761]\n",
      " [0.000 0.000 1.000]\n",
      " [0.010 0.001 0.989]\n",
      " [0.008 0.946 0.047]\n",
      " [0.001 0.976 0.023]\n",
      " [0.714 0.000 0.286]\n",
      " [0.003 0.000 0.997]\n",
      " [0.990 0.000 0.010]\n",
      " [0.000 0.000 1.000]\n",
      " [0.193 0.669 0.137]\n",
      " [0.601 0.000 0.399]\n",
      " [0.205 0.001 0.794]\n",
      " [0.000 0.000 1.000]\n",
      " [0.005 0.000 0.995]\n",
      " [0.000 0.000 1.000]\n",
      " [0.005 0.000 0.995]\n",
      " [0.601 0.000 0.399]\n",
      " [0.126 0.113 0.761]\n",
      " [0.010 0.001 0.989]\n",
      " [0.554 0.000 0.446]\n",
      " [0.205 0.001 0.794]\n",
      " [0.001 0.976 0.023]\n",
      " [0.000 0.000 1.000]\n",
      " [0.003 0.000 0.997]\n",
      " [0.000 0.000 1.000]\n",
      " [0.008 0.946 0.047]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(3)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.3,\n",
    "                                   height_shift_range=0.3,\n",
    "                                   shear_range=0.4,\n",
    "                                   zoom_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/train',\n",
    "        target_size=(24, 24),\n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'warehouse/hard_handwriting_shape/test',\n",
    "        target_size=(24, 24),    \n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(24,24,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "'''\n",
    "# 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(12, (5, 5), padding='same', activation='relu', input_shape=(24, 24, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(8, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, (2, 2), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "'''\n",
    "\n",
    "# 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습시키기\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=60,\n",
    "        epochs=200,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=15)\n",
    "\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            test_generator, \n",
    "            steps = 15)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "\n",
    "output = model.predict_generator(\n",
    "            test_generator, \n",
    "            steps = 15)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력되는 패치 이미지는 다음과 같습니다.\n",
    "\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_1.png)\n",
    "\n",
    "위의 패치 이미지를 입력으로 데이터 부풀리기를 한 결과는 다음과 같습니다.\n",
    "\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8-CNN_Data_Augmentation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 문제 정의하기\n",
    "\n",
    "좋은 예제와 그와 관련된 데이터셋도 공개된 것이 많이 있지만, 직접 문제를 정의하고 데이터를 만들어보는 것도 처럼 딥러닝을 접하시는 분들에게는 크게 도움이 될 것 같습니다. 컨볼루션 신경망 모델에 적합한 문제는 이미지 기반의 분류입니다. 따라서 우리는 직접 손으로 삼각형, 사각형, 원을 그려 이미지로 저장한 다음 이를 분류해보는 모델을 만들어보겠습니다. 문제 형태와 입출력을 다음과 같이 정의해봅니다.\n",
    "* 문제 형태 : 다중 클래스 분류\n",
    "* 입력 : 손으로 그린 삼각형, 사각형, 원 이미지\n",
    "* 출력 : 삼각형, 사각형, 원일 확률을 나타내는 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매번 실행 시마다 결과가 달라지지 않도록 랜덤 시드를 명시적으로 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 데이터셋 준비하기\n",
    "\n",
    "손으로 그린 삼각형, 사각형, 원 이미지를 만들기 위해서는 여러가지 방법이 있을 수 있겠네요. 테블릿을 이용할 수도 있고, 종이에 그려서 사진으로 찍을 수도 있습니다. 저는 그림 그리는 툴을 이용해서 만들어봤습니다. 이미지 사이즈는 24 x 24 정도로 해봤습니다. \n",
    "\n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8_CNN_Getting_Started_1.png)\n",
    "\n",
    "모양별로 20개 정도를 만들어서 15개를 훈련에 사용하고, 5개를 테스트에 사용해보겠습니다. 이미지는 png나 jpg로 저장합니다. 실제로 데이터셋이 어떻게 구성되어 있는 지 모른 체 튜토리얼을 따라하거나 예제 코드를 실행시키다보면 결과는 잘 나오지만 막상 실제 문제에 적용할 때 막막해질 때가 있습니다. 간단한 예제로 직접 데이터셋을 만들어봄으로써 실제 문제에 접근할 때 시행착오를 줄이는 것이 중요합니다.\n",
    "\n",
    "데이터셋 폴더는 다음과 같이 구성했습니다.\n",
    "\n",
    "- train\n",
    " - circle\n",
    " - rectangle\n",
    " - triangle\n",
    "- validation\n",
    " - circle\n",
    " - rectangle\n",
    " - triangle\n",
    " \n",
    "![data](http://tykimos.github.com/Keras/warehouse/2017-3-8_CNN_Getting_Started_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 데이터셋 불러오기\n",
    "\n",
    "케라스에서는 이미지 파일을 쉽게 학습시킬 수 있도록 ImageDataGenerator 클래스를 제공합니다. ImageDataGenerator 클래스는 데이터 증강 (data augmentation)을 위해 막강한 기능을 제공하는 데, 이 기능들은 다른 강좌에세 다루기로 하고, 본 강좌에서는 특정 폴더에 이미지를 분류 해놓았을 때 이를 학습시키기 위한 데이터셋으로 만들어주는 기능을 사용해보겠습니다.\n",
    "\n",
    "먼저 ImageDataGenerator 클래스를 이용하여 객체를 생성한 뒤 flow_from_directory() 함수를 호출하여 제네레이터(generator)를 생성합니다. flow_from_directory() 함수의 주요인자는 다음과 같습니다.\n",
    "\n",
    "- 첫번재 인자 : 이미지 경로를 지정합니다.\n",
    "- target_size : 패치 이미지 크기를 지정합니다. 폴더에 있는 원본 이미지 크기가 다르더라도 target_size에 지정된 크기로 자동 조절됩니다.\n",
    "- batch_size : 배치 크기를 지정합니다.\n",
    "- class_mode : 분류 방식에 대해서 지정합니다.\n",
    "    - categorical : 2D one-hot 부호화된 라벨이 반환됩니다.\n",
    "    - binary : 1D 이진 라벨이 반환됩니다.\n",
    "    - sparse : 1D 정수 라벨이 반환됩니다.\n",
    "    - None : 라벨이 반환되지 않습니다.\n",
    "\n",
    "본 예제에서는 패치 이미지 크기를 24 x 24로 하였으니 target_size도 (24, 24)로 셋팅하였습니다. 훈련 데이터 수가 클래스당 15개이니 배치 크기를 3으로 지정하여 총 5번 배치를 수행하면 하나의 epoch가 수행될 수 있도록 하였습니다. 다중 클래스 문제이므로 class_mode는 'categorical'로 지정하였습니다. 그리고 제네레이터는 훈련용과 검증용으로 두 개를 만들었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=40,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'warehouse/handwriting_shape/train',\n",
    "        target_size=(24, 24),\n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'warehouse/handwriting_shape/validation',\n",
    "        target_size=(24, 24),    \n",
    "        batch_size=3,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 모델 구성하기\n",
    "\n",
    "영상 분류에 높은 성능을 보이고 있는 컨볼루션 신경망 모델을 구성해보겠습니다. 이전 강좌에서 만들어봤던 모델 위에 입력을 24 x 24, 3개 채널 이미지를 받을 수 있고 필터를 12개를 가진 컨볼루션 레이어와 맥스풀링 레이어를 추가해봤습니다.\n",
    "\n",
    "* 컨볼루션 레이어 : 입력 이미지 크기 24 x 24, 입력 이미지 채널 3개, 필터 크기 3 x 3, 필터 수 12개, 경계 타입 'same', 활성화 함수 'relu'\n",
    "* 맥스풀링 레이어 : 풀 크기 2 x 2\n",
    "* 컨볼루션 레이어 : 입력 이미지 크기 8 x 8, 입력 이미지 채널 1개, 필터 크기 3 x 3, 필터 수 2개, 경계 타입 'same', 활성화 함수 'relu'\n",
    "* 맥스풀링 레이어 : 풀 크기 2 x 2\n",
    "* 컨볼루션 레이어 : 입력 이미지 크기 4 x 4, 입력 이미지 채널 2개, 필터 크기 2 x 2, 필터 수 3개, 경계 타입 'same', 활성화 함수 'relu'\n",
    "* 맥스풀링 레이어 : 풀 크기 2 x 2\n",
    "* 플래튼 레이어\n",
    "* 댄스 레이어 : 입력 뉴런 수 12개, 출력 뉴런 수 8개, 활성화 함수 'relu'\n",
    "* 댄스 레이어 : 입력 뉴런 수 8개, 출력 뉴런 수 3개, 활성화 함수 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(12, 3, 3, border_mode='same', input_shape=(3, 24, 24), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(2, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(3, 2, 2, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](http://tykimos.github.com/Keras/warehouse/2017-3-8_CNN_Getting_Started_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 모델 엮기\n",
    "\n",
    "모델을 정의했다면 모델을 손실함수와 최적화 알고리즘으로 엮어봅니다. \n",
    "- loss : 현재 가중치 세트를 평가하는 데 사용한 손실 함수 입니다. 다중 클래스 문제이므로 'categorical_crossentropy'으로 지정합니다.\n",
    "- optimizer : 최적의 가중치를 검색하는 데 사용되는 최적화 알고리즘으로 효율적인 경사 하강법 알고리즘 중 하나인 'adam'을 사용합니다.\n",
    "- metrics : 평가 척도를 나타내며 분류 문제에서는 일반적으로 'accuracy'으로 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습시키기\n",
    "\n",
    "케라스에서는 모델을 학습시킬 때 주로 fit 함수를 사용하지만 제네레이터로 생성된 배치로 학습시킬 경우에는 fit_generator 함수를 사용합니다. 본 예제에서는 ImageDataGenerator라는 제네레이터로 이미지를 담고 있는 배치로 학습시키기 때문에 fit_generator 함수를 사용하겠습니다.\n",
    "\n",
    "- 첫번째 인자 : 훈련데이터셋을 제공할 제네레이터를 지정합니다. 본 예제에서는 앞서 생성한 train_generator으로 지정합니다.\n",
    "- samples_per_epoch : 한 epoch에 사용한 샘플 수를 지정합니다. 총 45개의 훈련 샘플이 있으므로 45로 지정합니다.\n",
    "- nb_epoch : 전체 훈련 데이터셋에 대해 학습 반복 횟수를 지정합니다. 100번을 반복적으로 학습시켜 보겠습니다.\n",
    "- validation_data : 검증데이터셋을 제공할 제네레이터를 지정합니다. 본 예제에서는 앞서 생성한 validation_generator으로 지정합니다.\n",
    "- nb_val_samples : 한 epoch 종료 시 마다 검증할 때 사용되는 검증 샘플 수를 지정합니다. 홍 15개의 검증 샘플이 있으므로 15로 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 모델 학습시키기\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=45,\n",
    "        nb_epoch=100,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 모델 사용하기\n",
    "\n",
    "학습한 모델을 평가해봅니다. 제네레이터에서 제공되는 샘플로 평가할 때는 evaluate_generator 함수를 사용하고, 예측할 때는 predict_generator 함수를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            validation_generator, \n",
    "            val_samples = 15)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "\n",
    "output = model.predict_generator(\n",
    "            validation_generator, \n",
    "            val_samples = 15)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 모델이고 데이터셋이 적은 데도 불구하고 93.33%라는 높은 정확도를 얻었습니다. 개수로 따지면 검증 샘플 15개 중 1개가 잘 못 분류가 되었네요. Predict 함수는 입력된 이미지에 대해서 모델의 결과를 알려주는 역할을 하는 데, 각 샘플별로 클래스별 확률을 확인할 수 있습니다. 각 열은 다음을 뜻합니다.\n",
    "- 첫번째 열 : 원일 확률\n",
    "- 두번째 열 : 사각형일 확률\n",
    "- 세번째 열 : 삼각형일 확률\n",
    "\n",
    "확인을 해보니 rectangle020.png 파일이 사격형이 아니라 삼각형으로 판정이 되었습니다. 사각형을 그릴 때는 몰랐었는데, 막상 모델에서 삼각형이라고 얘기하니 다른 사각형이랑 조금 다르게 그린 것 같습니다.\n",
    "\n",
    "![predict](http://tykimos.github.com/Keras/warehouse/2017-3-8_CNN_Getting_Started_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 전체 소스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 결론\n",
    "\n",
    "본 강좌에서는 이미지 분류 문제를 직접 정의해보고 데이터셋도 직접 만들어봤습니다. 이미지 분류 문제에 높은 성능을 보이고 있는 컨볼루션 신경망 모델을 이용하여 직접 만든 데이터셋으로 학습 및 평가를 해보았습니다. 학습 결과는 좋게 나왔지만 이 모델은 한 사람이 그린 것에 대해서만 학습이 되어 있어서 다른 사람에 그린 모양은 잘 분류를 못할 것 같습니다. 이후 강좌에서는 다른 사람이 그린 모양으로 평가해보고 어떻게 모델 성능을 높일 수 있을 지 알아보겠습니다.\n",
    "\n",
    "그리고 실제 문제에 적용하기 전에 데이터셋을 직접 만들어보거나 좀 더 쉬운 문제로 추상화해서 프로토타이핑 하시는 것을 권장드립니다. 객담도말된 결핵 이미지 판별하는 모델을 만들 때, 결핵 이미지를 바로 사용하지 않고, MNIST의 손글씨 중 '1'과 '7'을 결핵이라고 보고, 나머지는 결핵이 아닌 것으로 학습시켜봤었습니다. 결핵균이 간균 (막대모양)이라 적절한 프로토타이핑이었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 같이 보기\n",
    "\n",
    "* [강좌 목차](https://tykimos.github.io/Keras/lecture/)\n",
    "* 이전 : [딥러닝 모델 이야기/컨볼루션 신경망 레이어 이야기](https://tykimos.github.io/Keras/2017/01/27/CNN_Layer_Talk/)\n",
    "* 다음 : [딥러닝 모델 이야기/순환 신경망 레이어 이야기]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
