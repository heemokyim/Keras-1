{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"수치입력 수치예측 모델 레시피\"\n",
    "author: 김태영\n",
    "date:   2017-08-13 23:10:00\n",
    "categories: Lecture\n",
    "comments: true\n",
    "image: http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_4m.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수치를 입력해서 수치를 예측하는 모델들에 대해서 알아보겠습니다. 수치예측을 위한 데이터셋 생성을 해보고, 선형회귀를 위한 가장 간단한 퍼셉트론 모델부터 깊은 다층퍼셉트론 모델까지 구성 및 학습을 시켜보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[7000:]\n",
    "y_val = y_train[7000:]\n",
    "x_train = x_train[:7000]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "#loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "#acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[7000:]\n",
    "y_val = y_train[7000:]\n",
    "x_train = x_train[:7000]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[7000:]\n",
    "y_val = y_train[7000:]\n",
    "x_train = x_train[:7000]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[7000:]\n",
    "y_val = y_train[7000:]\n",
    "x_train = x_train[:7000]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256,\n",
    "                 3,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[7000:]\n",
    "y_val = y_train[7000:]\n",
    "x_train = x_train[:7000]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256,\n",
    "                 3,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터셋 준비\n",
    "\n",
    "입력 x에 대해 2를 곱해 두 배 정도 값을 갖는 출력 y가 되도록 데이터셋을 생성해봤습니다. 선형회귀 모델을 사용한다면 Y = w * X + b 일 때, w가 2에 가깝고, b가 0.16에 가깝게 되도록 학습시키는 것이 목표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 데이터셋 생성\n",
    "x_train = np.random.random((1000, 1))\n",
    "y_train = x_train * 2 + np.random.random((1000, 1)) / 3.0\n",
    "x_test = np.random.random((100, 1))\n",
    "y_test = x_test * 2 + np.random.random((100, 1)) / 3.0\n",
    "\n",
    "# 데이터셋 확인\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_train, y_train, 'ro')\n",
    "plt.plot(x_test, y_test, 'bo')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 레이어 준비\n",
    "\n",
    "수치예측 모델에 사용할 레이어는 `Dense`와 `Activation`입니다. `Activation`에는 은닉층(hidden layer)에 사용할 `relu`를 준비했습니다. 데이터셋은 일차원 벡터만 다루도록 하겠습니다.\n",
    "\n",
    "|종류|구분|상세구분|브릭|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|데이터셋|Vector|-|![img](http://tykimos.github.com/Keras/warehouse/DeepBrick/Model_Recipe_Part_Dataset_Vector_s.png)|\n",
    "|레이어|Dense||![img](http://tykimos.github.com/Keras/warehouse/DeepBrick/Model_Recipe_Part_Dense_s.png)|\n",
    "|레이어|Activation|relu|![img](http://tykimos.github.com/Keras/warehouse/DeepBrick/Model_Recipe_Part_Activation_Relu_s.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 모델 준비\n",
    "\n",
    "수치예측을 하기 위해 `선형회귀 모델`, `퍼셉트론 모델`, `다층퍼셉트론 모델`, `깊은 다층퍼셉트론 모델`을 준비했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형회귀 모델\n",
    "\n",
    "가장 간단한 1차 선형회귀 모델로 수치예측을 해보겠습니다. 아래 식에서 x, y는 우리가 만든 데이터셋이고, 회귀분석을 통해서, w와 b값을 구하는 것이 목표입니다. \n",
    "\n",
    "    Y = w * X + b\n",
    "   \n",
    "w와 b값을 구하게 되면, 임의의 입력 x에 대해서 출력 y가 나오는 데 이것이 예측 값입니다. w, b 값은 분산, 공분산, 평균을 이용하여 쉽게 구할 수 있습니다. \n",
    "\n",
    "    w = np.cov(X, Y, bias=1)[0,1] / np.var(X)\n",
    "    b = np.average(Y) - w * np.average(X)\n",
    "    \n",
    "간단한 수식이지만 이 수식을 도출하기란 꽤나 복잡습니다. 오차를 최소화하는 극대값을 구하기 위해 편미분을 수행하고, 다시 식을 전개하는 등등의 과정이 필요합니다.\n",
    "\n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_0.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 퍼셉트론 모델\n",
    "\n",
    "Dense 레이어가 하나이고, 뉴런의 수도 하나인 가장 기본적인 퍼셉트론 모델입니다. 즉 웨이트(w) 하나, 바이어스(b) 하나로 전형적인 Y = w * X + b를 풀기 위한 모델입니다. 수치 예측을 하기 위해서 출력 레이어에 별도의 활성화 함수를 사용하지 않았습니다. w, b 값이 손으로 푼 선형회귀 최적해에 근접하려면 경우에 따라 만번이상의 에포크가 필요합니다. 실제로 사용하지는 않는 모델이지만 선형회귀부터 공부하시는 분들에게는 입문 모델로 나쁘지 않습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=1))\n",
    "        \n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_1m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다층퍼셉트론 모델\n",
    "\n",
    "Dense 레이어가 두 개인 다층퍼셉트론 모델입니다. 첫 번째 레이어는 64개의 뉴런을 가진 Dense 레이어이고 오류역전파가 용이한 `relu` 활성화 함수를 사용하였습니다. 출력 레이어인 두 번째 레이어는 하나의 수치값을 예측을 하기 위해서 1개의 뉴런을 가지며, 별도의 활성화 함수를 사용하지 않았습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_2m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 깊은 다층퍼셉트론 모델\n",
    "\n",
    "Dense 레이어가 총 세 개인 다층퍼셉트론 모델입니다. 첫 번째, 두 번째 레이어는 64개의 뉴런을 가진 Dense 레이어이고 오류역전파가 용이한 `relu` 활성화 함수를 사용하였습니다. 출력 레이어인 세 번째 레이어는 하나의 수치값을 예측을 하기 위해서 1개의 뉴런을 가지며, 별도의 활성화 함수를 사용하지 않았습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_3m.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 전체 소스\n",
    "\n",
    "앞서 살펴본 `선형회귀 모델`, `퍼셉트론 모델`, `다층퍼셉트론 모델`, `깊은 다층퍼셉트론 모델`의 전체 소스는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다층퍼셉트론 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층퍼셉트론 모델로 수치예측하기\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "x_train = np.random.random((1000, 1))\n",
    "y_train = x_train * 2 + np.random.random((1000, 1)) / 3.0\n",
    "x_test = np.random.random((100, 1))\n",
    "y_test = x_test * 2 + np.random.random((100, 1)) / 3.0\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=50, batch_size=64)\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('loss : ' + str(loss))\n",
    "\n",
    "# 6. 학습과정 확인하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.ylim(0.0, 1.5)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 깊은 다층퍼셉트론 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깊은 다층퍼셉트론 모델로 수치예측하기\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "x_train = np.random.random((1000, 1))\n",
    "y_train = x_train * 2 + np.random.random((1000, 1)) / 3.0\n",
    "x_test = np.random.random((100, 1))\n",
    "y_test = x_test * 2 + np.random.random((100, 1)) / 3.0\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=50, batch_size=64)\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('loss : ' + str(loss))\n",
    "\n",
    "# 6. 학습과정 확인하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.ylim(0.0, 1.5)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 학습결과 비교\n",
    "\n",
    "퍼셉트론 > 다층퍼셉트론 > 깊은 다층퍼셉트론 순으로 학습이 좀 더 빨리 되는 것을 확인할 수 있습니다.\n",
    "\n",
    "|퍼셉트론|다층퍼셉트론|깊은 다층퍼셉트론|\n",
    "|:-:|:-:|:-:|\n",
    "|![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_6.png)|![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_7.png)|![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_8.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 결론\n",
    "\n",
    "수치예측을 위한 퍼셉트론, 다층퍼셉트론, 깊은 다층퍼셉트론 모델을 살펴보고, 그 성능을 확인 해봤습니다.\n",
    "\n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_4m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 같이 보기\n",
    "\n",
    "* [강좌 목차](https://tykimos.github.io/Keras/lecture/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
