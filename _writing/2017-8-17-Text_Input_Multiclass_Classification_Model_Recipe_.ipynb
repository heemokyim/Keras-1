{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"수치입력 수치예측 모델 레시피\"\n",
    "author: 김태영\n",
    "date:   2017-08-13 23:10:00\n",
    "categories: Lecture\n",
    "comments: true\n",
    "image: http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_4m.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수치를 입력해서 수치를 예측하는 모델들에 대해서 알아보겠습니다. 수치예측을 위한 데이터셋 생성을 해보고, 선형회귀를 위한 가장 간단한 퍼셉트론 모델부터 깊은 다층퍼셉트론 모델까지 구성 및 학습을 시켜보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1982 samples\n",
      "Epoch 1/15\n",
      "7000/7000 [==============================] - 5s - loss: 1.9771 - acc: 0.5129 - val_loss: 1.4997 - val_acc: 0.6504\n",
      "Epoch 2/15\n",
      "7000/7000 [==============================] - 4s - loss: 0.9189 - acc: 0.7904 - val_loss: 1.3328 - val_acc: 0.6953\n",
      "Epoch 3/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.2966 - acc: 0.9494 - val_loss: 1.4032 - val_acc: 0.6776\n",
      "Epoch 4/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1865 - acc: 0.9613 - val_loss: 1.4121 - val_acc: 0.6736\n",
      "Epoch 5/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1558 - acc: 0.9623 - val_loss: 1.4022 - val_acc: 0.6862\n",
      "Epoch 6/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1409 - acc: 0.9610 - val_loss: 1.3987 - val_acc: 0.6892\n",
      "Epoch 7/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1294 - acc: 0.9626 - val_loss: 1.4600 - val_acc: 0.6675\n",
      "Epoch 8/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1200 - acc: 0.9627 - val_loss: 1.4031 - val_acc: 0.6927\n",
      "Epoch 9/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1072 - acc: 0.9631 - val_loss: 1.4417 - val_acc: 0.6927\n",
      "Epoch 10/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.1012 - acc: 0.9644 - val_loss: 1.4495 - val_acc: 0.6897\n",
      "Epoch 11/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.0954 - acc: 0.9636 - val_loss: 1.4896 - val_acc: 0.6776\n",
      "Epoch 12/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.0901 - acc: 0.9633 - val_loss: 1.5009 - val_acc: 0.6826\n",
      "Epoch 13/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.0869 - acc: 0.9623 - val_loss: 1.4381 - val_acc: 0.6988\n",
      "Epoch 14/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.0816 - acc: 0.9624 - val_loss: 1.5413 - val_acc: 0.6867\n",
      "Epoch 15/15\n",
      "7000/7000 [==============================] - 5s - loss: 0.0761 - acc: 0.9624 - val_loss: 1.5087 - val_acc: 0.6968\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvGg555wrDC9azjnnCuP/z/xLdaLwR6kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1108a1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112/2246 [===========================>..] - ETA: 0s## evaluation loss and_metrics ##\n",
      "[1.5619765644294283, 0.67586821015138021]\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[7000:]\n",
    "y_val = y_train[7000:]\n",
    "x_train = x_train[:7000]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "#loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "#acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256,\n",
    "                 3,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "max_features = 20000\n",
    "text_max_words = 80\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "# 데이터셋 전처리 : # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=text_max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=text_max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=text_max_words)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=text_max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256,\n",
    "                 3,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([-0.2, 1.2])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "#yhat_test = model.predict(xhat_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터셋 준비\n",
    "\n",
    "입력 x에 대해 2를 곱해 두 배 정도 값을 갖는 출력 y가 되도록 데이터셋을 생성해봤습니다. 선형회귀 모델을 사용한다면 Y = w * X + b 일 때, w가 2에 가깝고, b가 0.16에 가깝게 되도록 학습시키는 것이 목표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 데이터셋 생성\n",
    "x_train = np.random.random((1000, 1))\n",
    "y_train = x_train * 2 + np.random.random((1000, 1)) / 3.0\n",
    "x_test = np.random.random((100, 1))\n",
    "y_test = x_test * 2 + np.random.random((100, 1)) / 3.0\n",
    "\n",
    "# 데이터셋 확인\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_train, y_train, 'ro')\n",
    "plt.plot(x_test, y_test, 'bo')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 레이어 준비\n",
    "\n",
    "수치예측 모델에 사용할 레이어는 `Dense`와 `Activation`입니다. `Activation`에는 은닉층(hidden layer)에 사용할 `relu`를 준비했습니다. 데이터셋은 일차원 벡터만 다루도록 하겠습니다.\n",
    "\n",
    "|종류|구분|상세구분|브릭|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|데이터셋|Vector|-|![img](http://tykimos.github.com/Keras/warehouse/DeepBrick/Model_Recipe_Part_Dataset_Vector_s.png)|\n",
    "|레이어|Dense||![img](http://tykimos.github.com/Keras/warehouse/DeepBrick/Model_Recipe_Part_Dense_s.png)|\n",
    "|레이어|Activation|relu|![img](http://tykimos.github.com/Keras/warehouse/DeepBrick/Model_Recipe_Part_Activation_Relu_s.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 모델 준비\n",
    "\n",
    "수치예측을 하기 위해 `선형회귀 모델`, `퍼셉트론 모델`, `다층퍼셉트론 모델`, `깊은 다층퍼셉트론 모델`을 준비했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형회귀 모델\n",
    "\n",
    "가장 간단한 1차 선형회귀 모델로 수치예측을 해보겠습니다. 아래 식에서 x, y는 우리가 만든 데이터셋이고, 회귀분석을 통해서, w와 b값을 구하는 것이 목표입니다. \n",
    "\n",
    "    Y = w * X + b\n",
    "   \n",
    "w와 b값을 구하게 되면, 임의의 입력 x에 대해서 출력 y가 나오는 데 이것이 예측 값입니다. w, b 값은 분산, 공분산, 평균을 이용하여 쉽게 구할 수 있습니다. \n",
    "\n",
    "    w = np.cov(X, Y, bias=1)[0,1] / np.var(X)\n",
    "    b = np.average(Y) - w * np.average(X)\n",
    "    \n",
    "간단한 수식이지만 이 수식을 도출하기란 꽤나 복잡습니다. 오차를 최소화하는 극대값을 구하기 위해 편미분을 수행하고, 다시 식을 전개하는 등등의 과정이 필요합니다.\n",
    "\n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_0.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 퍼셉트론 모델\n",
    "\n",
    "Dense 레이어가 하나이고, 뉴런의 수도 하나인 가장 기본적인 퍼셉트론 모델입니다. 즉 웨이트(w) 하나, 바이어스(b) 하나로 전형적인 Y = w * X + b를 풀기 위한 모델입니다. 수치 예측을 하기 위해서 출력 레이어에 별도의 활성화 함수를 사용하지 않았습니다. w, b 값이 손으로 푼 선형회귀 최적해에 근접하려면 경우에 따라 만번이상의 에포크가 필요합니다. 실제로 사용하지는 않는 모델이지만 선형회귀부터 공부하시는 분들에게는 입문 모델로 나쁘지 않습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=1))\n",
    "        \n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_1m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다층퍼셉트론 모델\n",
    "\n",
    "Dense 레이어가 두 개인 다층퍼셉트론 모델입니다. 첫 번째 레이어는 64개의 뉴런을 가진 Dense 레이어이고 오류역전파가 용이한 `relu` 활성화 함수를 사용하였습니다. 출력 레이어인 두 번째 레이어는 하나의 수치값을 예측을 하기 위해서 1개의 뉴런을 가지며, 별도의 활성화 함수를 사용하지 않았습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_2m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 깊은 다층퍼셉트론 모델\n",
    "\n",
    "Dense 레이어가 총 세 개인 다층퍼셉트론 모델입니다. 첫 번째, 두 번째 레이어는 64개의 뉴런을 가진 Dense 레이어이고 오류역전파가 용이한 `relu` 활성화 함수를 사용하였습니다. 출력 레이어인 세 번째 레이어는 하나의 수치값을 예측을 하기 위해서 1개의 뉴런을 가지며, 별도의 활성화 함수를 사용하지 않았습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_3m.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 전체 소스\n",
    "\n",
    "앞서 살펴본 `선형회귀 모델`, `퍼셉트론 모델`, `다층퍼셉트론 모델`, `깊은 다층퍼셉트론 모델`의 전체 소스는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다층퍼셉트론 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 다층퍼셉트론 모델로 수치예측하기\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "x_train = np.random.random((1000, 1))\n",
    "y_train = x_train * 2 + np.random.random((1000, 1)) / 3.0\n",
    "x_test = np.random.random((100, 1))\n",
    "y_test = x_test * 2 + np.random.random((100, 1)) / 3.0\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=50, batch_size=64)\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('loss : ' + str(loss))\n",
    "\n",
    "# 6. 학습과정 확인하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.ylim(0.0, 1.5)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 깊은 다층퍼셉트론 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 깊은 다층퍼셉트론 모델로 수치예측하기\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "x_train = np.random.random((1000, 1))\n",
    "y_train = x_train * 2 + np.random.random((1000, 1)) / 3.0\n",
    "x_test = np.random.random((100, 1))\n",
    "y_test = x_test * 2 + np.random.random((100, 1)) / 3.0\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=50, batch_size=64)\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('loss : ' + str(loss))\n",
    "\n",
    "# 6. 학습과정 확인하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.ylim(0.0, 1.5)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 학습결과 비교\n",
    "\n",
    "퍼셉트론 > 다층퍼셉트론 > 깊은 다층퍼셉트론 순으로 학습이 좀 더 빨리 되는 것을 확인할 수 있습니다.\n",
    "\n",
    "|퍼셉트론|다층퍼셉트론|깊은 다층퍼셉트론|\n",
    "|:-:|:-:|:-:|\n",
    "|![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_6.png)|![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_7.png)|![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_8.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 결론\n",
    "\n",
    "수치예측을 위한 퍼셉트론, 다층퍼셉트론, 깊은 다층퍼셉트론 모델을 살펴보고, 그 성능을 확인 해봤습니다.\n",
    "\n",
    "![img](http://tykimos.github.com/Keras/warehouse/2017-8-12-Numerical_Prediction_Model_Recipe_4m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 같이 보기\n",
    "\n",
    "* [강좌 목차](https://tykimos.github.io/Keras/lecture/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
