{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"데이터셋과 학습과정 이야기\"\n",
    "author: Taeyoung, Kim\n",
    "date:   2017-03-25 12:00:00\n",
    "categories: Keras\n",
    "comments: true\n",
    "image: http://tykimos.github.com/Keras/warehouse/2017-3-25-Dataset_and_Fit_Talk_1.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 강좌에서는 텐서보드(TensorBoard)를 사용해서 학습 과정을 시각화하는 방법에 대해서 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 학습 과정을 시각화하는 이유\n",
    "\n",
    "저는 이렇게 이해했습니다. 어차피 학습과정이니 시험치는 것에 비유해봤습니다.\n",
    "\n",
    "- 모의고사 1부에 100문항이 있다고 가정합니다.\n",
    "- epoch는 같은 모의고사를 몇 번 풀 것이냐 입니다. epoch가 10이면 같은 모의고사지를 10번 푸는 것과 동일합니다. 처음 epoch를 접할 때는 동일한 데이터셋을 여러번 사용하는 것이 도움이 돼?라고 생각했는데, 같은 모의고사지를 여러번 푼다고 생각해보니, 점점 학습이 될 것 같더군요.\n",
    "- batch size는 몇 문항을 보고 답을 맞춰볼까 입니다. 실전처럼 하는 사람은 전 문항을 다 풀고 난 다음 답을 맞춰서 오답정리를 하는 사람도 있고 성격이 급한 사람은 한 문제 풀고 답 맞추는 사람도 있겠죠. 처음엔 이것도 학습에 무슨영향을 미칠까 의아해 했는데, 같은 모의고사지에 비슷한 문제가 있을 경우 다 풀어보고 맞춘 사람은 한 문제 틀리면 다른 유사문데들도 다 틀리겠지만, 한 문항보고 오답정리한 사람은 첫 문제를 틀리더라도 다른 남은 문제들은 맞출 확률이 높아지겠죠. \n",
    "- 그밖에 train, validation, test셋을 왜 나누어야 하는 지도 모의고사, 실전 모의고사, 실제 수능 등으로 생각해보니 당연히 그렇게 해야겠구나 생각이 들더군요.\n",
    "\n",
    "비유가 이상하거나 제가 잘 못 이해하고 있다면 지적해주세요~\n",
    "\n",
    "\n",
    "검증셋은 학습에 영향을 미치지 않는다.\n",
    "학습은 문제와 답을 준 상태에서 학생이 문제를 풀고 답을 맞춰보는 과정에서 이루어 집니다. 이 때 학생이 아하~ 하면서 이해하는 것이 weight를 업데이트 하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 케라스 전용 시각화 툴\n",
    "\n",
    "[검증셋 이야기]\n",
    "데이터셋에 관한 글을 읽다가 제가 얼마나 무지했던가?를 깨닫고 정리해봤습니다. 기초적인 내용이라서 다들 잘 아시겠지만 입문하시는 분들에게는 도움이 되실까 올려봅니다.\n",
    "# 처음엔\n",
    "- 검증셋이 훈련 시에 사용되기에 back-propagation 시 weight에 영향을 미치는 줄 알았습니다.\n",
    "- 같은 이유로 k-fold cross validation 하면 좀 더 검증된 모델이 훈련되는 줄 알았습니다.\n",
    "- 테스트셋으로 평가해서 좋은 모델을 선택했습니다.\n",
    "# 하지만 \n",
    "- 검증셋은 훈련 시 모델에 영향을 미치지 않습니다. 단 보통 훈련셋에서 일부를 검증셋으로 사용하기 때문에 검증셋의 선택을 바꾸면, 훈련셋이 바뀌기 때문에 영향을 미칠 수는 있습니다만 이것은 검증셋 때문이라기 보다 훈련셋의 변화로 영향을 미치는 것입니다. 또한 과적합을 피하기 위해 조기종료를 할 경우 검증셋의 에러율을 이용하기 때문에 이때는 모델 훈련에 영향을 미칠 수 있겠네요.\n",
    "- k-fold cross validation 시 검증셋이 바뀔 때 마다(iteration시 마다) 학습된 모델을 사용하는 것이 아니라, 모델의 weight는 초기화해야하고, 각 fold에서 나오는 검증셋의 에러율을 평균하여 해당 모델의 에러율을 판정합니다. 하나의 검증셋보다 여러 개의 검증셋으로 검증하고, 각 검증 시에는 처음부터 훈련을 시켜야 검증에 대한 신뢰성이 높아집니다.\n",
    "- 모델 선택은 테스트셋의 평가가 아닌 검증셋의 평가로 이루어집니다. 모델 선택이란 여러 후보 모델이 있을 경우, 검증셋을 통해서 가장 낮은 에러를 가진 모델을 최종 모델로 선택하는 것을 말합니다. 여러 후보 모델이란 의미는 SVM, CNN 등 모델 자체가 다른 것도 있겠지만, 같은 모델이라고 하더라도 설정된 파라미터가 다른 경우에도 적용됩니다. 이를 하이퍼파라미터 튜닝이라고 하는 데, CNN인 경우에는 필터 크기, 개수 등이 있을 수 있겠네요. 즉 필터의 설정을 바꿔가면서 훈련 시키고, 그 중 좋은 파라미터를 선택할 때 사용되는 것이 검증셋입니다. 그러니 k-fold cross validation을 사용하게되면, 튜닝된 모델 선택 시에 좀 더 검증된 모델을 선택할 수 있는 것입니다. GridSearchCV() 함수 예제를 보니, CNN에서의 하이퍼파라미터을 찾는 것이 상상이 되더군요. (필터 크기를 바꿔가면서, 필터 개수를 바꿔가면서, epoch를 바꿔가면서 등등 가장 좋은 모델을 선택하는 코드를 봤습니다만 그저 상상에 그쳤습니다. 해보기엔 지금 장비로 엄두가 나지 않네요. )\n",
    "# 아직 확신이 들지 않는 것은\n",
    "- 검증셋을 통해 모델이 선택되면 그대로 사용하는 것이 아니라 모델을 전체 훈련셋으로 다시 훈련을 시켜라는 코멘트도 봤는데, 과적합이 되지 않는 epoch에서 전체 훈련셋으로 훈련할 경우, 추가되는 훈련셋(기존에 검증셋으로 사용했던) 때문에 과적합이 생기는 건 아닌 지 염려스럽네요. (미비할 것이라고 추측은 됩니다만)\n",
    "- 훈련셋이 적을 경우 데이터 업샘플링(회전, 확대/축소, 각종 변형)을 하는 데, 검증셋에도 적용해도 될까요? k-fold cross validation을 하게 되면 업샘플링된 훈련셋에서 검증셋을 선정하기 때문에 검증셋도 업샘플링된 데이터로 구성이 되긴 하는데, 바람직한지는 잘 모르겠습니다. 검증셋은 업샘플링 등의 과정이 없이 최대한 원본 그대로를 유지해야 좀 더 신뢰성이 높을 것 같습니다.\n",
    "잘못된 정보가 있다면 코멘트 부탁드리겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 텐서보드(TensorBoard)란\n",
    "\n",
    "Dong-Hyun Lee Competition처럼 결과에 매우 민감한 상황에는 valid set으로 hyperparameter tuning을 한 후에 그걸 train set에 넣어 다시 training하는 것이 도움이 될 것 같습니다만, training sample수가 충분한 경우에는 구지 그렇게까지 할 필요가 있을까 싶습니다. / upsampling 과정이 reasonable 하다면 upsampling test set도 나름 의미가 있다고 생각이 듭니다만, 다른 모델과 정확한 비교를 하기 위해서는 완전히 같은 test set을 이용해야 할 것 같습니다.\n",
    "좋아요 취소 · 답글 달기 · 2 · 2016년 8월 22일 오전 2:22 · 수정됨\n",
    "김태영\n",
    "김태영 의견 감사합니다~!\n",
    "좋아요 · 답글 달기 · 2016년 8월 22일 오전 6:36\n",
    "\n",
    "\n",
    "문창기 제 경우 보행자 검출시 초기 훈련 set을 육안으로 변별력 있게 잘 정제한 후 훈련된 분류기를 테스트 set에 적용하여 나온 false positive와 false negative를 retraining하는 방식으로 분류기의 성능 강화를 진행중에 있습니다.\n",
    "좋아요 취소 · 답글 달기 · 1 · 2016년 8월 22일 오전 8:24\n",
    "김태영\n",
    "김태영 성능 강화에 좋은 아이디어이신것 같습니다. 테스트셋을 통해 FP, FN의 데이터를 훈련시킨 다음에는 테스트셋에서 제외 시키는 건가요? 훈련시킨 데이터가 훈련셋에도 있고 테스트셋에도 있으면 과적합이 될 것 같습니다.\n",
    "좋아요 · 답글 달기 · 2016년 8월 22일 오후 5:04\n",
    "문창기\n",
    "문창기 김태영 분류기의 일반화 능력이 어느정도 생긴 다음부터는 훈련 셋과 테스트 셋이 중복되더라도 오버피팅은 발생하지 않았습니다.\n",
    "좋아요 취소 · 답글 달기 · 1 · 2016년 8월 22일 오후 10:11\n",
    "\n",
    "문창기 분류기의 정확성도 중요하지만 속도를 고려해야 하기에 훈련 및 테스트 셋에 일정부분 사람의 개입을 의도적으로 해주고 있습니다.\n",
    "좋아요 취소 · 답글 달기 · 2 · 2016년 8월 22일 오전 8:28\n",
    "김태영\n",
    "김태영 어떤 식으로 사람 개입이 되는지 궁금합니다. 저도 사람이 판정한 훈련셋의 신뢰도를 높이기 위해, 모델 결과 중 FP, FN을 다시 사람에게 보여줘서 재판독 하는 과정을 넣어서 내가 훈련시킨 모델을 이용하여 훈련셋 정제에 사용하고자 하는데, 이런식의 개입인가요?\n",
    "좋아요 · 답글 달기 · 2016년 8월 22일 오후 5:06\n",
    "문창기\n",
    "문창기 김태영 db를 구성하는 방법에 있어서 개입을 말씀 드렸습니다. Occlusion 도 추가시키고, 랜덤 노이즈와 랜덤 블러링도 살짝씩 추가시키구요. 물론 초기부터 적용하는게 아니라, 적어도 주어진 테스트 셋에 대해서 일반화 능력이 어느정도 만들어진 다음부터 테스트 셋을 위와 같이 인위적 개입을 통해 강화시키고 있습니다.\n",
    "좋아요 취소 · 답글 달기 · 1 · 2016년 8월 22일 오후 10:17\n",
    "\n",
    "\n",
    "Dohyeong Kim 전체 데이터 세트를 k-fold cross-validation을 이용하여 나누고 grid search를 이용하여 파라미터를 바꾸어 가면서 계산하여 최종적으로 나오는 최적 모델은 하나일텐데 이 모델을 다른 데이터 세트로 훈련 한다는 말인가요?\n",
    "좋아요 취소 · 답글 달기 · 1 · 2016년 8월 22일 오후 7:02 · 수정됨\n",
    "김태영\n",
    "김태영 제가 본 코멘트에서는 훈련셋이 부족해서 최적모델이 선택되면 같은 파라미터로 검증셋까지 훈련시키는 것 같았습니다. 하지만 말씀하신 것처럼 최적 모델이 나오면 더이상 업데이트를 안 하는 것이 맞을 것 같습니다.\n",
    "좋아요 · 답글 달기 · 1 · 2016년 8월 23일 오전 8:26 · 수정됨\n",
    "\n",
    "\n",
    "박세진 가장 낮은 에러를 가지는 모델이 항상 좋지는 않은것같습니다\n",
    "또 k fold나 subsampling이 만드는 불균일성 문제도 있는것 같고요\n",
    "Examplar svm형태의 hinge loss를 가지는 모델의 앙상블도 고려해봄직한데\n",
    "데이터개수의 모델을 만들어야 한다는 현실적문제도 있고요\n",
    "저도 이걸로 계속 고민중인데 아직 정답근처도 못간것 같습니다\n",
    "가장 큰 문제는 negative data의 분포를 모델링하기가 너무 어렵다는 것\n",
    "fp,fn정제 및 stage 마다 갱신하는 학습방법도 어느정도 데이터에선 효과적이었습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 케라스와 텐서보드를 연동하는 법\n",
    "\n",
    "Tensorboard basic visualizations.\n",
    "\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.\n",
    "\n",
    "TensorBoard is a visualization tool provided with TensorFlow.\n",
    "\n",
    "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n",
    "\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "You can find more information about TensorBoard - __here.\n",
    "\n",
    "Arguments\n",
    "\n",
    "* log_dir : the path of the directory where to save the log files to be parsed by Tensorboard\n",
    "* histogram_freq: frequency (in epochs) at which to compute activation histograms for the layers of the model. If set to 0, histograms won't be computed.\n",
    "* write_graph: whether to visualize the graph in Tensorboard. The log file can become quite large when write_graph is set to True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: TensorBoard callback only works with the TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n",
      "INFO:tensorflow:Summary name convolution2d_1_W:0 is illegal; using convolution2d_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "INFO:tensorflow:Summary name convolution2d_1_W:0 is illegal; using convolution2d_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_1_W:0 is illegal; using convolution2d_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_1_W:0 is illegal; using convolution2d_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_1_b:0 is illegal; using convolution2d_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_1_b:0 is illegal; using convolution2d_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_1_b:0 is illegal; using convolution2d_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_1_b:0 is illegal; using convolution2d_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_W:0 is illegal; using convolution2d_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_W:0 is illegal; using convolution2d_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_W:0 is illegal; using convolution2d_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_W:0 is illegal; using convolution2d_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_b:0 is illegal; using convolution2d_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_b:0 is illegal; using convolution2d_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_b:0 is illegal; using convolution2d_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_2_b:0 is illegal; using convolution2d_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_W:0 is illegal; using convolution2d_3_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_W:0 is illegal; using convolution2d_3_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_W:0 is illegal; using convolution2d_3_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_W:0 is illegal; using convolution2d_3_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_b:0 is illegal; using convolution2d_3_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_b:0 is illegal; using convolution2d_3_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_b:0 is illegal; using convolution2d_3_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name convolution2d_3_b:0 is illegal; using convolution2d_3_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_W:0 is illegal; using dense_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_W:0 is illegal; using dense_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_W:0 is illegal; using dense_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_W:0 is illegal; using dense_2_W_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_b:0 is illegal; using dense_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_b:0 is illegal; using dense_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_b:0 is illegal; using dense_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dense_2_b:0 is illegal; using dense_2_b_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "45/45 [==============================] - 0s - loss: 8.5139 - acc: 0.3778 - val_loss: 7.7609 - val_acc: 0.4000\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s - loss: 7.5305 - acc: 0.4444 - val_loss: 6.6558 - val_acc: 0.4667\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s - loss: 5.5426 - acc: 0.6000 - val_loss: 5.6512 - val_acc: 0.5333\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s - loss: 5.4888 - acc: 0.6000 - val_loss: 5.7667 - val_acc: 0.5333\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s - loss: 5.1460 - acc: 0.6222 - val_loss: 5.2414 - val_acc: 0.3333\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s - loss: 2.1311 - acc: 0.4222 - val_loss: 1.6423 - val_acc: 0.2000\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s - loss: 1.0828 - acc: 0.3556 - val_loss: 1.1054 - val_acc: 0.2667\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s - loss: 1.0840 - acc: 0.3556 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s - loss: 1.0881 - acc: 0.3556 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s - loss: 1.0758 - acc: 0.3556 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s - loss: 1.0631 - acc: 0.3778 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s - loss: 1.0571 - acc: 0.3778 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s - loss: 1.0761 - acc: 0.3778 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s - loss: 1.0767 - acc: 0.3556 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s - loss: 1.0992 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s - loss: 1.0992 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s - loss: 1.0991 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s - loss: 1.0989 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s - loss: 1.0988 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "-- Evaluate --\n",
      "acc: 33.33%\n",
      "-- Predict --\n",
      "[[0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]\n",
      " [0.333 0.333 0.334]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(5)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'warehouse/handwriting_shape/train',\n",
    "        target_size=(24, 24),\n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'warehouse/handwriting_shape/validation',\n",
    "        target_size=(24, 24),    \n",
    "        batch_size=3,\n",
    "        class_mode='categorical')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(12, 3, 3, border_mode='same', input_shape=(3, 24, 24), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(2, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(3, 2, 2, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# 텐서보드 연동하기\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "# 모델 학습시키기\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=45,\n",
    "        nb_epoch=200,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=15,\n",
    "        callbacks=[tensorboard_callback])\n",
    "\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            validation_generator, \n",
    "            val_samples = 15)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    "\n",
    "output = model.predict_generator(\n",
    "            validation_generator, \n",
    "            val_samples = 15)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 결론\n",
    "\n",
    "본 강좌에서는 이미지 분류 문제를 직접 정의해보고 데이터셋도 직접 만들어봤습니다. 이미지 분류 문제에 높은 성능을 보이고 있는 컨볼루션 신경망 모델을 이용하여 직접 만든 데이터셋으로 학습 및 평가를 해보았습니다. 학습 결과는 좋게 나왔지만 이 모델은 한 사람이 그린 것에 대해서만 학습이 되어 있어서 다른 사람에 그린 모양은 잘 분류를 못할 것 같습니다. 이후 강좌에서는 다른 사람이 그린 모양으로 평가해보고 어떻게 모델 성능을 높일 수 있을 지 알아보겠습니다.\n",
    "\n",
    "그리고 실제 문제에 적용하기 전에 데이터셋을 직접 만들어보거나 좀 더 쉬운 문제로 추상화해서 프로토타이핑 하시는 것을 권장드립니다. 객담도말된 결핵 이미지 판별하는 모델을 만들 때, 결핵 이미지를 바로 사용하지 않고, MNIST의 손글씨 중 '1'과 '7'을 결핵이라고 보고, 나머지는 결핵이 아닌 것으로 학습시켜봤었습니다. 결핵균이 간균 (막대모양)이라 적절한 프로토타이핑이었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 같이 보기\n",
    "\n",
    "* [강좌 목차](https://tykimos.github.io/Keras/2017/01/27/Keras_Lecture_Plan/)\n",
    "* 이전 : [딥러닝 모델 이야기/컨볼루션 신경망 레이어 이야기](https://tykimos.github.io/Keras/2017/01/27/CNN_Layer_Talk/)\n",
    "* 다음 : [딥러닝 모델 이야기/순환 신경망 레이어 이야기]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
